{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length in cm</th>\n",
       "      <th>sepal width in cm</th>\n",
       "      <th>petal length in cm</th>\n",
       "      <th>petal width in cm</th>\n",
       "      <th>class label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length in cm  sepal width in cm  petal length in cm  \\\n",
       "145                 6.7                3.0                 5.2   \n",
       "146                 6.3                2.5                 5.0   \n",
       "147                 6.5                3.0                 5.2   \n",
       "148                 6.2                3.4                 5.4   \n",
       "149                 5.9                3.0                 5.1   \n",
       "\n",
       "     petal width in cm     class label  \n",
       "145                2.3  Iris-virginica  \n",
       "146                1.9  Iris-virginica  \n",
       "147                2.0  Iris-virginica  \n",
       "148                2.3  Iris-virginica  \n",
       "149                1.8  Iris-virginica  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "The link to the OG doc for LDA: https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/discriminant_analysis.py\n",
    "'''\n",
    "\n",
    "feature_dict = {i:label for i,label in zip(\n",
    "                range(4),\n",
    "                  ('sepal length in cm',\n",
    "                  'sepal width in cm',\n",
    "                  'petal length in cm',\n",
    "                  'petal width in cm', ))}\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.io.parsers.read_csv(\n",
    "    filepath_or_buffer='https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data',\n",
    "    header=None,\n",
    "    sep=',',\n",
    "    )\n",
    "df.columns = [l for i,l in sorted(feature_dict.items())] + ['class label']\n",
    "df.dropna(how=\"all\", inplace=True) # to drop the empty line at file-end\n",
    "\n",
    "df.tail()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.1  3.5  1.4  0.2] 150 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X = df[['sepal length in cm','sepal width in cm','petal length in cm','petal width in cm']].values\n",
    "y = df['class label'].values\n",
    "\n",
    "\n",
    "\n",
    "enc = LabelEncoder()\n",
    "label_encoder = enc.fit(y)\n",
    "y = label_encoder.transform(y) + 1\n",
    "\n",
    "label_dict = {1: 'Setosa', 2: 'Versicolor', 3:'Virginica'}\n",
    "print(X[0],len(y),y)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "# LDA\n",
    "sklearn_lda = LDA(n_components=3)\n",
    "X_lda_sklearn = sklearn_lda.fit_transform(X, y)\n",
    "print(np.shape(X_lda_sklearn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_scikit_lda(X, title):\n",
    "\n",
    "    ax = plt.subplot(111)\n",
    "    for label,marker,color in zip(\n",
    "        range(1,4),('^', 's', 'o'),('blue', 'red', 'green')):\n",
    "\n",
    "        plt.scatter(x=X[:,0][y == label],\n",
    "                    y=X[:,1][y == label] * -1, # flip the figure\n",
    "                    marker=marker,\n",
    "                    color=color,\n",
    "                    alpha=0.5,\n",
    "                    label=label_dict[label])\n",
    "\n",
    "    plt.xlabel('LD1')\n",
    "    plt.ylabel('LD2')\n",
    "\n",
    "    leg = plt.legend(loc='upper right', fancybox=True)\n",
    "    leg.get_frame().set_alpha(0.5)\n",
    "    plt.title(title)\n",
    "\n",
    "    # hide axis ticks\n",
    "    plt.tick_params(axis=\"both\", which=\"both\", bottom=\"off\", top=\"off\",  \n",
    "            labelbottom=\"on\", left=\"off\", right=\"off\", labelleft=\"on\")\n",
    "\n",
    "    # remove axis spines\n",
    "    ax.spines[\"top\"].set_visible(False)  \n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines[\"bottom\"].set_visible(False)\n",
    "    ax.spines[\"left\"].set_visible(False)    \n",
    "\n",
    "    plt.grid()\n",
    "    plt.tight_layout\n",
    "    plt.show()\n",
    "    \n",
    "def plot_step_lda():\n",
    "\n",
    "    ax = plt.subplot(111)\n",
    "    for label,marker,color in zip(\n",
    "        range(1,4),('^', 's', 'o'),('blue', 'red', 'green')):\n",
    "\n",
    "        plt.scatter(x=X_lda_sklearn[:,0].real[y == label],\n",
    "                y=X_lda_sklearn[:,1].real[y == label],\n",
    "                marker=marker,\n",
    "                color=color,\n",
    "                alpha=0.5,\n",
    "                label=label_dict[label]\n",
    "                )\n",
    "\n",
    "    plt.xlabel('LD1')\n",
    "    plt.ylabel('LD2')\n",
    "\n",
    "    leg = plt.legend(loc='upper right', fancybox=True)\n",
    "    leg.get_frame().set_alpha(0.5)\n",
    "    plt.title('LDA: Iris projection onto the first 2 linear discriminants')\n",
    "\n",
    "    # hide axis ticks\n",
    "    plt.tick_params(axis=\"both\", which=\"both\", bottom=\"off\", top=\"off\",  \n",
    "            labelbottom=\"on\", left=\"off\", right=\"off\", labelleft=\"on\")\n",
    "\n",
    "    # remove axis spines\n",
    "    ax.spines[\"top\"].set_visible(False)  \n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines[\"bottom\"].set_visible(False)\n",
    "    ax.spines[\"left\"].set_visible(False)    \n",
    "\n",
    "    plt.grid()\n",
    "    plt.tight_layout\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEWCAYAAABmE+CbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFEVRAFUIiqIoisP/AwXqgf4QcVYNAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11817b7b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plot_scikit_lda(X_lda_sklearn, title='Default LDA via scikit-learn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "EmptyDataError",
     "evalue": "No columns to parse from file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmptyDataError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-19f21951efb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'LDA_img_ratio_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_early_late_all.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m#was '_view_all.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     )#,skiprows=10,nrows=10\n\u001b[1;32m     42\u001b[0m \u001b[0;31m#df=df.loc[df['Myr'] == 5 or df['Myr'] == 225]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mEmptyDataError\u001b[0m: No columns to parse from file"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "feature_dict = {i:label for i,label in zip(\n",
    "                range(14),\n",
    "                  ('Counter',\n",
    "                  'Image',\n",
    "                  'class label',\n",
    "                  'Myr',\n",
    "                  'Viewpoint',\n",
    "                '# Bulges',\n",
    "                   'Sep',\n",
    "                   'Flux Ratio',\n",
    "                  'Gini',\n",
    "                  'M20',\n",
    "                  'Concentration (C)',\n",
    "                  'Asymmetry (A)',\n",
    "                  'Clumpiness (S)',\n",
    "                  'Sersic N',\n",
    "                  'Shape Asymmetry (A_S)'))}\n",
    "\n",
    "#Counter\tImage\tMerger (0 = no, 1 = yes)\tMyr\tViewpoint\tGini\tM20\tC\tA\tS\tSersic n\n",
    "'''view=0\n",
    "df = pd.io.parsers.read_table(\n",
    "    filepath_or_buffer='PCA_img_0.txt',\n",
    "    header=[0],\n",
    "    sep='\\t', skiprows=14*view,nrows=14\n",
    "    )#,skiprows=10,nrows=10'''\n",
    "run='fg3_m_12'\n",
    "\n",
    "#LDA_img_ratio_fg3_m12_early_late_all.txt\n",
    "myr=5,200,180,185,190,195,205,210,220,225,230,240,250,260\n",
    "df2 = pd.io.parsers.read_table(\n",
    "    filepath_or_buffer='LDA_img_ratio_'+str(run)+'_early_late_all.txt',#was '_view_all.txt'\n",
    "    header=[0],\n",
    "    sep='\\t'\n",
    "    )#,skiprows=10,nrows=10\n",
    "#df=df.loc[df['Myr'] == 5 or df['Myr'] == 225]\n",
    "#df=df.loc[df['Myr'].isin([5,200,myr])]\n",
    "df2.columns = [l for i,l in sorted(feature_dict.items())] + ['Shape Asymmetry']\n",
    "df2.dropna(how=\"all\", inplace=True) # to drop the empty line at file-end\n",
    "\n",
    "list_kins=['NONSCATTER_MILESHC_corrected','SCATTER_MILESHC_corrected','SCATTER','NONSCATTER']\n",
    "\n",
    "for i in range(len(list_kins)):\n",
    "   \n",
    "    add_on=list_kins[i]\n",
    "    print('run', add_on)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    feature_dict = {i:label for i,label in zip(\n",
    "                    range(9),\n",
    "                      ('Counter',\n",
    "                       'Myr',\n",
    "                      'Viewpoint',\n",
    "                      'Delta PA',\n",
    "                      'v_asym',\n",
    "                      's_asym',\n",
    "                      'K_tot',\n",
    "                      'resids',\n",
    "                      'i'))}\n",
    "\n",
    "    \n",
    "    df = pd.io.parsers.read_table(\n",
    "        filepath_or_buffer='PCA_kin_'+str(add_on)+'.txt',\n",
    "        header=[0],\n",
    "        sep='\\t'\n",
    "        )#,skiprows=10,nrows=10\n",
    "    #df=df.loc[df['Myr'] == 5 or df['Myr'] == 225]\n",
    "    #df=df.loc[df['Myr'].isin([5,200,myr])]\n",
    "    df.columns = [l for i,l in sorted(feature_dict.items())] + ['fiber']\n",
    "    df.dropna(how=\"all\", inplace=True) # to drop the empty line at file-end\n",
    "\n",
    "\n",
    "\n",
    "    merged = pd.merge(left=df2,right=df, on=['Myr','Viewpoint'])\n",
    "    if i==4:\n",
    "        print(merged)\n",
    "\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "    X = merged[['Gini','M20','Concentration (C)', 'Asymmetry (A)', 'Clumpiness (S)', 'Sersic N', 'Shape Asymmetry',\n",
    "       'v_asym','s_asym']].values\n",
    "\n",
    "    from sklearn import preprocessing\n",
    "\n",
    "\n",
    "\n",
    "    std_scale = preprocessing.StandardScaler().fit(X)\n",
    "    X = std_scale.transform(X)\n",
    "\n",
    "\n",
    "    n_params=9\n",
    "\n",
    "\n",
    "    y = merged['class label'].values\n",
    "\n",
    "\n",
    "    enc = LabelEncoder()\n",
    "    label_encoder = enc.fit(y)\n",
    "    y = label_encoder.transform(y) + 1\n",
    "\n",
    "\n",
    "    label_dict = {1: 'NonMerger', 2: 'Merger'}\n",
    "    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "    # LDA\n",
    "    sklearn_lda = LDA(priors=[0.8,0.2])\n",
    "    X_lda_sklearn = sklearn_lda.fit_transform(X, y)\n",
    "    dec = sklearn_lda.score(X,y)\n",
    "    \n",
    "    coef = sklearn_lda.coef_\n",
    "    inter = sklearn_lda.intercept_\n",
    "    class_label = sklearn_lda.classes_\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(dec)#mean accuracy on the given test data and labels.\n",
    "    print(coef)\n",
    "    print(inter)\n",
    "    \n",
    "    from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "    \n",
    "    # QDA\n",
    "    sklearn_qda = QDA(priors=[0.8,0.2])\n",
    "    X_qda_sklearn = sklearn_qda.fit(X, y)\n",
    "    dec_qda = sklearn_qda.score(X,y)\n",
    "    \n",
    "    #coef = sklearn_qda.coef_\n",
    "    #inter = sklearn_qda.intercept_\n",
    "    print(dec_qda)#mean accuracy on the given test data and labels.\n",
    "\n",
    "    '''Make a histogram'''\n",
    "    from scipy import stats\n",
    "    import seaborn as sns\n",
    "    plt.clf()\n",
    "    fig = plt.figure(figsize=(18,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    X_lda_1=[]\n",
    "    X_lda_2=[]\n",
    "    for j in range(len(X_lda_sklearn)):\n",
    "        if y[j] ==1:\n",
    "            X_lda_1.append(X_lda_sklearn[j][0])\n",
    "        else:\n",
    "            X_lda_2.append(X_lda_sklearn[j][0])\n",
    "    input_hist=X_lda_sklearn\n",
    "    \n",
    "    ax.hist(X_lda_1, label='NonMerger',  color=sns.xkcd_rgb[\"sky blue\"])\n",
    "    ax.hist(X_lda_2, label='Merger',  color=sns.xkcd_rgb[\"salmon\"])\n",
    "\n",
    "    '''for label,col in zip(range(1,4),  ('blue', 'red')):\n",
    "        input_hist=X_lda_sklearn\n",
    "        input_all=X_lda_sklearn\n",
    "        ax.hist(input_hist,\n",
    "                       color=col,\n",
    "                       label='class %s' %label_dict[label],\n",
    "                       alpha=0.5,)#bins=bins,\n",
    "        xt = plt.xticks()[0]  \n",
    "        xmin, xmax = -0.1,0.7#min(xt), max(xt)  \n",
    "        lnspc = np.linspace(xmin, xmax, len(input_hist))\n",
    "\n",
    "        # lets try the normal distribution first\n",
    "        m, s = stats.norm.fit(input_hist) # get mean and standard deviation  \n",
    "        pdf_g = stats.norm.pdf(lnspc, m, s) # now get theoretical values in our interval  \n",
    "        #ax.plot(lnspc, pdf_g,  color=col) # plot it\n",
    "\n",
    "\n",
    "\n",
    "    ylims = ax.get_ylim()\n",
    "\n",
    "    # plot annotation\n",
    "    leg = ax.legend(loc='upper right', fancybox=True, fontsize=8)\n",
    "    leg.get_frame().set_alpha(0.5)\n",
    "    ax.set_ylim([0, max(ylims)+2])'''\n",
    "\n",
    "    ax.set_xlabel('LD1', size=20)\n",
    "    #ax.set_title('Histogram #%s' %str(cnt+1), size=20)\n",
    "\n",
    "    # hide axis ticks\n",
    "    ax.tick_params(axis=\"both\", which=\"both\", bottom=\"off\", top=\"off\",  \n",
    "            labelbottom=\"on\", left=\"off\", right=\"off\", labelleft=\"on\", labelsize=20)\n",
    "\n",
    "    # remove axis spines\n",
    "    ax.spines[\"top\"].set_visible(False)  \n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines[\"bottom\"].set_visible(False)\n",
    "    ax.spines[\"left\"].set_visible(False)    \n",
    "\n",
    "    ax.set_ylabel('Count', size=20)\n",
    "    \n",
    "    \n",
    "    plt.legend(loc=\"upper right\", fontsize=20)\n",
    "    #fig.tight_layout() \n",
    "    #plt.annotate(str(add_on), xy=(0.02,0.95),xycoords='axes fraction', size=20)\n",
    "    #plt.annotate('Mean Accuracy = '+str(dec), xy=(0.02,0.9),xycoords='axes fraction', size=20)\n",
    "    plt.show()\n",
    "    \n",
    "#    savefig('../MaNGA_Papers/Paper_I/Bayesian_Hist_'+str(run)+'.pdf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCATTER -0.746634884\n",
      "NONSCATTER -0.69609521\n",
      "NONSCATTER_MILESHC_corrected -1.08988571\n",
      "SCATTER_MILESHC_corrected -0.81003349\n",
      "SCATTER 0.00265348228\n",
      "NONSCATTER -0.0078832\n",
      "NONSCATTER_MILESHC_corrected 0.02130446\n",
      "SCATTER_MILESHC_corrected -0.05767831\n",
      "SCATTER 4.05597665\n",
      "NONSCATTER 4.41491687\n",
      "NONSCATTER_MILESHC_corrected 4.17536727\n",
      "SCATTER_MILESHC_corrected 4.0528699\n",
      "SCATTER 6.91605057\n",
      "NONSCATTER 6.15332374\n",
      "NONSCATTER_MILESHC_corrected 6.55635034\n",
      "SCATTER_MILESHC_corrected 6.7326127\n",
      "SCATTER -3.6524561\n",
      "NONSCATTER -3.04132293\n",
      "NONSCATTER_MILESHC_corrected -3.41241524\n",
      "SCATTER_MILESHC_corrected -3.51244676\n",
      "SCATTER 0.68381577\n",
      "NONSCATTER 0.66753479\n",
      "NONSCATTER_MILESHC_corrected 1.34479293\n",
      "SCATTER_MILESHC_corrected 0.93881014\n",
      "SCATTER 0.785409412\n",
      "NONSCATTER 0.68833139\n",
      "NONSCATTER_MILESHC_corrected 0.55429217\n",
      "SCATTER_MILESHC_corrected 0.72658846\n",
      "SCATTER -0.769581706\n",
      "NONSCATTER -0.35440716\n",
      "NONSCATTER_MILESHC_corrected -1.72265744\n",
      "SCATTER_MILESHC_corrected -1.10888531\n",
      "SCATTER -0.0519556937\n",
      "NONSCATTER 0.80938349\n",
      "NONSCATTER_MILESHC_corrected 0.74637557\n",
      "SCATTER_MILESHC_corrected 0.27643255\n",
      "SCATTER vs SCATTER 0.0\n",
      "SCATTER vs NONSCATTER 8.420205374440267\n",
      "SCATTER vs NONSCATTER_MILESHC_corrected 9.87600044880428\n",
      "SCATTER vs SCATTER_MILESHC_corrected 3.7351776955937104\n",
      "NONSCATTER vs SCATTER 8.420205374440267\n",
      "Domain error NONSCATTER vs NONSCATTER\n",
      "NONSCATTER vs NONSCATTER 8.420205374440267\n",
      "NONSCATTER vs NONSCATTER_MILESHC_corrected 10.61252383355065\n",
      "NONSCATTER vs SCATTER_MILESHC_corrected 7.869113853338481\n",
      "NONSCATTER_MILESHC_corrected vs SCATTER 9.87600044880428\n",
      "NONSCATTER_MILESHC_corrected vs NONSCATTER 10.612523833550583\n",
      "NONSCATTER_MILESHC_corrected vs NONSCATTER_MILESHC_corrected 0.0\n",
      "NONSCATTER_MILESHC_corrected vs SCATTER_MILESHC_corrected 6.239535022517412\n",
      "SCATTER_MILESHC_corrected vs SCATTER 3.73517769559332\n",
      "SCATTER_MILESHC_corrected vs NONSCATTER 7.869113853338481\n",
      "SCATTER_MILESHC_corrected vs NONSCATTER_MILESHC_corrected 6.239535022517296\n",
      "SCATTER_MILESHC_corrected vs SCATTER_MILESHC_corrected 1.2074182697257333e-06\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "NONSCATTER=[-0.69609521, -0.0078832,   4.41491687,  6.15332374, -3.04132293,  0.66753479,\n",
    "   0.68833139, -0.35440716,  0.80938349]\n",
    "#A,C,S,sigma,G,A_S,n,vel,M20\n",
    "#1/10th\n",
    "SCATTER=[ -7.46634884e-01,   2.65348228e-03 ,  4.05597665e+00 ,  6.91605057e+00,\n",
    "   -3.65245610e+00 ,  6.83815770e-01 ,  7.85409412e-01,  -7.69581706e-01,\n",
    "   -5.19556937e-02]\n",
    "#A,C,S,A_S,vel,Gini,n,sigma,M20\n",
    "#1/10th\n",
    "SCATTER_MILESHC_corrected=[-0.81003349, -0.05767831 , 4.0528699  , 6.7326127,  -3.51244676,  0.93881014,\n",
    "   0.72658846, -1.10888531,  0.27643255]\n",
    "#A,C,S,vel,n,G,A_S,sigma,M20\n",
    "#OOM\n",
    "NONSCATTER_MILESHC_corrected=[-1.08988571,  0.02130446,  4.17536727,  6.55635034, -3.41241524,  1.34479293,\n",
    "   0.55429217, -1.72265744 , 0.74637557]\n",
    "#A,C,S,vel,n,Gini,sigma,A_S,M20\n",
    "#OOM\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "list_names=[SCATTER,NONSCATTER,NONSCATTER_MILESHC_corrected,SCATTER_MILESHC_corrected]\n",
    "act_names=['SCATTER','NONSCATTER','NONSCATTER_MILESHC_corrected','SCATTER_MILESHC_corrected']\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "for x in range(len(list_names)):\n",
    "    for y in range(len(list_names)):\n",
    "        try:\n",
    "            exp=math.degrees(math.acos(np.dot(list_names[x],list_names[y])/(np.linalg.norm(list_names[x])*np.linalg.norm(list_names[y]))))\n",
    "        except ValueError:\n",
    "            print('Domain error', act_names[x], 'vs', act_names[y])\n",
    "        if exp <90:\n",
    "            expnow=exp\n",
    "        else:\n",
    "            expnow=abs(180-exp)\n",
    "        print(act_names[x], 'vs', act_names[y], expnow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run fg3_m15\n",
      "~~~~Means PRE standardization~~~~\n",
      "[1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0] (117,)\n",
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/beckynevin/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:200: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "117/117 [==============================] - 1s 9ms/step - loss: 0.6319 - acc: 0.6410\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 2/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.5618 - acc: 0.7778\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 3/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.4798 - acc: 0.8376\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 4/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.4003 - acc: 0.8547\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 5/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.3364 - acc: 0.8632\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 6/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.2984 - acc: 0.8632\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 7/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.2716 - acc: 0.8547\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 8/20\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.2512 - acc: 0.8718\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 9/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.2344 - acc: 0.8718\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 10/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.2190 - acc: 0.8718\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 11/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.2045 - acc: 0.8974\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 12/20\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.1898 - acc: 0.9060\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 13/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1815 - acc: 0.9231\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 14/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1707 - acc: 0.9231\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 15/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1621 - acc: 0.9402\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 16/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1547 - acc: 0.9316\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 17/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1480 - acc: 0.9316\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 18/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1441 - acc: 0.9231\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 19/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1412 - acc: 0.9402\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 20/20\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.1377 - acc: 0.9402\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "30/30 [==============================] - 0s 9ms/step\n",
      "score [0.08308084309101105, 1.0]\n",
      "~~~~~Stratified K-fold validation means  ~~~~~~\n",
      "[[-0.17597112 -0.92947108  2.00445806  0.67540766  0.4059623   2.05240085]]\n",
      "[-1.63678659]\n",
      "~~~~~Stratified K-fold validation STD ~~~~~~\n",
      "[[0.19927724 0.13830811 0.31520957 0.1768719  0.12971943 0.24845291]]\n",
      "[0.10871145]\n",
      "~~~~~Comparing to last run~~~~~\n",
      "[[-0.62987782 -1.04310445  2.36982741  1.06707154  0.53160945  1.7386278 ]]\n",
      "[-1.75408838]\n",
      "~~~~~Master Confusion~~~~~\n",
      "[[5.6 4.8]\n",
      " [0.  4.3]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/beckynevin/anaconda/lib/python3.6/site-packages/matplotlib/pyplot.py:537: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n",
      "/Users/beckynevin/anaconda/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:656: DeprecationWarning: The parameter 'store_covariances' is deprecated as of version 0.17 and will be removed in 0.19. The parameter is no longer necessary because the value is set via the estimator initialisation or set_params method.\n",
      "  \"set_params method.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean accuracy 0.6802721088435374\n",
      "[-1.75408838]\n",
      "confusion matrix QDA [[5 2]\n",
      " [0 7]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1      0.714     1.000     0.833         5\n",
      "          2      1.000     0.778     0.875         9\n",
      "\n",
      "avg / total      0.898     0.857     0.860        14\n",
      "\n",
      "0.9251700680272109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/beckynevin/anaconda/lib/python3.6/site-packages/matplotlib/cbook/deprecation.py:107: MatplotlibDeprecationWarning: Passing one of 'on', 'true', 'off', 'false' as a boolean is deprecated; use an actual boolean (True/False) instead.\n",
      "  warnings.warn(message, mplDeprecation, stacklevel=1)\n",
      "/Users/beckynevin/anaconda/lib/python3.6/site-packages/matplotlib/cbook/deprecation.py:107: MatplotlibDeprecationWarning: Passing one of 'on', 'true', 'off', 'false' as a boolean is deprecated; use an actual boolean (True/False) instead.\n",
      "  warnings.warn(message, mplDeprecation, stacklevel=1)\n",
      "/Users/beckynevin/anaconda/lib/python3.6/site-packages/matplotlib/cbook/deprecation.py:107: MatplotlibDeprecationWarning: Passing one of 'on', 'true', 'off', 'false' as a boolean is deprecated; use an actual boolean (True/False) instead.\n",
      "  warnings.warn(message, mplDeprecation, stacklevel=1)\n",
      "/Users/beckynevin/anaconda/lib/python3.6/site-packages/matplotlib/cbook/deprecation.py:107: MatplotlibDeprecationWarning: Passing one of 'on', 'true', 'off', 'false' as a boolean is deprecated; use an actual boolean (True/False) instead.\n",
      "  warnings.warn(message, mplDeprecation, stacklevel=1)\n",
      "/Users/beckynevin/anaconda/lib/python3.6/site-packages/matplotlib/cbook/deprecation.py:107: MatplotlibDeprecationWarning: Passing one of 'on', 'true', 'off', 'false' as a boolean is deprecated; use an actual boolean (True/False) instead.\n",
      "  warnings.warn(message, mplDeprecation, stacklevel=1)\n",
      "/Users/beckynevin/anaconda/lib/python3.6/site-packages/matplotlib/cbook/deprecation.py:107: MatplotlibDeprecationWarning: Passing one of 'on', 'true', 'off', 'false' as a boolean is deprecated; use an actual boolean (True/False) instead.\n",
      "  warnings.warn(message, mplDeprecation, stacklevel=1)\n",
      "/Users/beckynevin/anaconda/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/beckynevin/anaconda/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/Users/beckynevin/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:212: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     class label\n",
      "0              0\n",
      "1              0\n",
      "2              0\n",
      "3              0\n",
      "4              0\n",
      "5              0\n",
      "6              0\n",
      "7              0\n",
      "8              1\n",
      "9              1\n",
      "10             1\n",
      "11             1\n",
      "12             1\n",
      "13             1\n",
      "14             1\n",
      "15             1\n",
      "16             1\n",
      "17             1\n",
      "18             1\n",
      "19             1\n",
      "20             1\n",
      "21             0\n",
      "22             0\n",
      "23             0\n",
      "24             0\n",
      "25             0\n",
      "26             0\n",
      "27             0\n",
      "28             0\n",
      "29             1\n",
      "..           ...\n",
      "117            1\n",
      "118            1\n",
      "119            1\n",
      "120            1\n",
      "121            1\n",
      "122            1\n",
      "123            1\n",
      "124            1\n",
      "125            1\n",
      "126            0\n",
      "127            0\n",
      "128            0\n",
      "129            0\n",
      "130            0\n",
      "131            0\n",
      "132            0\n",
      "133            0\n",
      "134            1\n",
      "135            1\n",
      "136            1\n",
      "137            1\n",
      "138            1\n",
      "139            1\n",
      "140            1\n",
      "141            1\n",
      "142            1\n",
      "143            1\n",
      "144            1\n",
      "145            1\n",
      "146            1\n",
      "\n",
      "[147 rows x 1 columns]\n",
      "myr [5, 30, 60, 100, 120, 150, 180, 200, 210, 240, 270, 300, 320, 340, 360, 400, 420]\n",
      "separations [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 6.407587375483571, 0.0, 7.266495274175582, 1.139780019987178, 0.0, 0.0, 0.0]\n",
      "means [1.49071413 0.93824562 1.26703976 1.03406064 1.39403906 1.17666776\n",
      " 3.57450094 2.40545149 2.05922399 2.52831801 2.50341987 2.6547393\n",
      " 2.1763636 ]\n",
      "run fg3_m12\n",
      "~~~~Means PRE standardization~~~~\n",
      "[1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1] (153,)\n",
      "[[0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "Epoch 1/20\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.6584 - acc: 0.6471  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 2/20\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.4839 - acc: 0.8431\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 3/20\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.3168 - acc: 0.9542\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 4/20\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1866 - acc: 0.9869\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 5/20\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.0993 - acc: 0.9869\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 6/20\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.0690 - acc: 0.9804\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 7/20\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.0595 - acc: 0.9869\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 8/20\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.0536 - acc: 0.9804\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 9/20\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.0520 - acc: 0.9804\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 10/20\n",
      "153/153 [==============================] - 0s 3ms/step - loss: 0.0483 - acc: 0.9804\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 11/20\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0460 - acc: 0.9804\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 12/20\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0449 - acc: 0.9804\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 13/20\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0411 - acc: 0.9804\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 14/20\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0399 - acc: 0.9804\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 15/20\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0370 - acc: 0.9804\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 16/20\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0353 - acc: 0.9804\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 17/20\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0333 - acc: 0.9935\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 18/20\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0312 - acc: 0.9935\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 19/20\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0302 - acc: 0.9869\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 20/20\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0289 - acc: 0.9935\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "39/39 [==============================] - 0s 8ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "score [0.13992429550928184, 0.9743589743589743]\n",
      "~~~~~Stratified K-fold validation means  ~~~~~~\n",
      "[[ 3.37761798 -1.16378     0.67053273  2.10795748  1.98634493  1.77803902]]\n",
      "[0.36817424]\n",
      "~~~~~Stratified K-fold validation STD ~~~~~~\n",
      "[[0.16055875 0.2753071  0.14521916 0.27906921 0.34976501 0.10031492]]\n",
      "[0.14518777]\n",
      "~~~~~Comparing to last run~~~~~\n",
      "[[ 3.24019748 -1.11927655  0.5732461   1.86073029  2.01101963  1.85434823]]\n",
      "[0.21129619]\n",
      "~~~~~Master Confusion~~~~~\n",
      "[[ 6.   1.6]\n",
      " [ 0.1 11.5]]\n",
      "mean accuracy 0.9166666666666666\n",
      "[0.21129619]\n",
      "confusion matrix QDA [[ 6  0]\n",
      " [ 0 13]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1      1.000     1.000     1.000         6\n",
      "          2      1.000     1.000     1.000        13\n",
      "\n",
      "avg / total      1.000     1.000     1.000        19\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/beckynevin/anaconda/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:656: DeprecationWarning: The parameter 'store_covariances' is deprecated as of version 0.17 and will be removed in 0.19. The parameter is no longer necessary because the value is set via the estimator initialisation or set_params method.\n",
      "  \"set_params method.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9947916666666666\n",
      "     class label\n",
      "0              0\n",
      "1              0\n",
      "2              0\n",
      "3              0\n",
      "4              0\n",
      "5              1\n",
      "6              1\n",
      "7              1\n",
      "8              1\n",
      "9              1\n",
      "10             1\n",
      "11             1\n",
      "12             1\n",
      "13             1\n",
      "14             1\n",
      "15             1\n",
      "16             1\n",
      "17             0\n",
      "18             0\n",
      "19             0\n",
      "20             0\n",
      "21             1\n",
      "22             1\n",
      "23             1\n",
      "24             1\n",
      "25             1\n",
      "26             1\n",
      "27             1\n",
      "28             0\n",
      "29             0\n",
      "..           ...\n",
      "162            1\n",
      "163            1\n",
      "164            1\n",
      "165            1\n",
      "166            0\n",
      "167            0\n",
      "168            0\n",
      "169            0\n",
      "170            0\n",
      "171            1\n",
      "172            1\n",
      "173            1\n",
      "174            1\n",
      "175            1\n",
      "176            1\n",
      "177            1\n",
      "178            1\n",
      "179            1\n",
      "180            1\n",
      "181            1\n",
      "182            1\n",
      "183            0\n",
      "184            0\n",
      "185            0\n",
      "186            1\n",
      "187            1\n",
      "188            1\n",
      "189            1\n",
      "190            1\n",
      "191            1\n",
      "\n",
      "[192 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/beckynevin/anaconda/lib/python3.6/site-packages/matplotlib/cbook/deprecation.py:107: MatplotlibDeprecationWarning: Passing one of 'on', 'true', 'off', 'false' as a boolean is deprecated; use an actual boolean (True/False) instead.\n",
      "  warnings.warn(message, mplDeprecation, stacklevel=1)\n",
      "/Users/beckynevin/anaconda/lib/python3.6/site-packages/matplotlib/cbook/deprecation.py:107: MatplotlibDeprecationWarning: Passing one of 'on', 'true', 'off', 'false' as a boolean is deprecated; use an actual boolean (True/False) instead.\n",
      "  warnings.warn(message, mplDeprecation, stacklevel=1)\n",
      "/Users/beckynevin/anaconda/lib/python3.6/site-packages/matplotlib/cbook/deprecation.py:107: MatplotlibDeprecationWarning: Passing one of 'on', 'true', 'off', 'false' as a boolean is deprecated; use an actual boolean (True/False) instead.\n",
      "  warnings.warn(message, mplDeprecation, stacklevel=1)\n",
      "/Users/beckynevin/anaconda/lib/python3.6/site-packages/matplotlib/cbook/deprecation.py:107: MatplotlibDeprecationWarning: Passing one of 'on', 'true', 'off', 'false' as a boolean is deprecated; use an actual boolean (True/False) instead.\n",
      "  warnings.warn(message, mplDeprecation, stacklevel=1)\n",
      "/Users/beckynevin/anaconda/lib/python3.6/site-packages/matplotlib/cbook/deprecation.py:107: MatplotlibDeprecationWarning: Passing one of 'on', 'true', 'off', 'false' as a boolean is deprecated; use an actual boolean (True/False) instead.\n",
      "  warnings.warn(message, mplDeprecation, stacklevel=1)\n",
      "/Users/beckynevin/anaconda/lib/python3.6/site-packages/matplotlib/cbook/deprecation.py:107: MatplotlibDeprecationWarning: Passing one of 'on', 'true', 'off', 'false' as a boolean is deprecated; use an actual boolean (True/False) instead.\n",
      "  warnings.warn(message, mplDeprecation, stacklevel=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myr [5, 10, 20, 30, 40, 60, 80, 100, 120, 140, 160, 170, 180, 185, 190, 195, 200, 205, 210, 220, 225, 230, 240, 250, 260]\n",
      "separations [14.726184250322929, 10.84627893322307, 1.4102673354148787, 0.0, 0.0, 0.0, 0.0, 0.0, 13.840352055061505, 9.291815545587506, 8.861595710151407, 4.058653018425777, 5.103486498601828, 10.039331265933978, 1.2526582030691606, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "means [1.94185621 1.29439185 3.8486333  4.24522184 2.99649863 2.97078994\n",
      " 2.76507902 3.4996898  5.83881976 3.19546748 3.40652352 4.46377568\n",
      " 4.40720015 4.27084942 2.21976106 3.44013375 2.85438649 3.54406545\n",
      " 3.35877973 3.08419265 2.91536335]\n",
      "run fg1_m13\n",
      "~~~~Means PRE standardization~~~~\n",
      "[1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] (166,)\n",
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n",
      "Epoch 1/20\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "~~~\n",
    "Now just for the imaging part of it!\n",
    "~~~\n",
    "'''\n",
    "\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "\n",
    "   # from http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#example-model-selection-plot-confusion-matrix-py\n",
    "def plot_confusion_matrix(cm, target_names, title, cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(target_names))\n",
    "    plt.xticks(tick_marks, target_names-1, rotation=45)\n",
    "    plt.yticks(tick_marks, target_names-1)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "\n",
    "def lda_classify(v, levels, cutoffpoints):\n",
    "    for level, cutoff in zip(reversed(levels), reversed(cutoffpoints)):\n",
    "        if v > cutoff: return level\n",
    "    return levels[0]\n",
    "\n",
    "'''def box_m(X0,X1):\n",
    "\n",
    "        global Xp\n",
    "\n",
    "        m = 2\n",
    "        k = len((X0))\n",
    "        n_1 = len(X0[0])\n",
    "        n_2 = len(X1[0])\n",
    "        n = len(X0[0])+len(X1[0])\n",
    "        \n",
    "        print(( ((n_1-1)*(X0)) + ((n_2-1)*(X1)) ))\n",
    "\n",
    "        Xp = ( ((n_1-1)*(X0)) + ((n_2-1)*(X1)) ) / (n-m)\n",
    "\n",
    "        M = ((n-m)*np.log(np.linalg.det(Xp))) \\\n",
    "         - (n_1-1)*(np.log(np.linalg.det((X0)))) - (n_2-1)*(np.log(np.linalg.det((X1))))\n",
    "\n",
    "        c = ( ( 2*(k**2) + (3*k) - 1 ) / ( (6*(k+1)*(m-1)) ) ) \\\n",
    "            * ( (1/(n_1-1)) + (1/(n_2-1)) - (1/(n-m)) )\n",
    "\n",
    "        df = (k*(k+1)*(m-1))/2\n",
    "\n",
    "        c2 = ( ((k-1)*(k+2)) / (6*(m-1)) ) \\\n",
    "            * ( (1/((n_1-1)**2)) + (1/((n_2-1)**2)) - (1/((n-m)**2)) )\n",
    "\n",
    "        df2 = (df+2) / (np.abs(c2-c**2))\n",
    "\n",
    "        if (c2>c**2):\n",
    "\n",
    "            a_plus = df / (1-c-(df/df2))\n",
    "\n",
    "            F = M / a_plus\n",
    "\n",
    "        else:\n",
    "\n",
    "            a_minus = df2 / (1-c+(2/df2))\n",
    "\n",
    "            F = (df2*M) / (df*(a_minus-M))\n",
    "\n",
    "        print('M = {}'.format(M))\n",
    "        print('c = {}'.format(c))\n",
    "        print('c2 = {}'.format(c2))\n",
    "        print('-------------------')\n",
    "        print('df = {}'.format(df))\n",
    "        print('df2 = {}'.format(df2))\n",
    "        print('-------------------')\n",
    "        print('F = {}'.format(F)) '''\n",
    "\n",
    "def box_m(X0,X1):\n",
    "\n",
    "        global Xp\n",
    "\n",
    "        m = 2\n",
    "        k = len(np.cov(X0))\n",
    "        n_1 = len(X0[0])\n",
    "        n_2 = len(X1[0])\n",
    "        n = len(X0[0])+len(X1[0])\n",
    "\n",
    "        Xp = ( ((n_1-1)*np.cov(X0)) + ((n_2-1)*np.cov(X1)) ) / (n-m)\n",
    "\n",
    "        M = ((n-m)*np.log(np.linalg.det(Xp))) \\\n",
    "         - (n_1-1)*(np.log(np.linalg.det(np.cov(X0)))) - (n_2-1)*(np.log(np.linalg.det(np.cov(X1))))\n",
    "\n",
    "        c = ( ( 2*(k**2) + (3*k) - 1 ) / ( (6*(k+1)*(m-1)) ) ) \\\n",
    "            * ( (1/(n_1-1)) + (1/(n_2-1)) - (1/(n-m)) )\n",
    "\n",
    "        df = (k*(k+1)*(m-1))/2\n",
    "\n",
    "        c2 = ( ((k-1)*(k+2)) / (6*(m-1)) ) \\\n",
    "            * ( (1/((n_1-1)**2)) + (1/((n_2-1)**2)) - (1/((n-m)**2)) )\n",
    "\n",
    "        df2 = (df+2) / (np.abs(c2-c**2))\n",
    "\n",
    "        if (c2>c**2):\n",
    "\n",
    "            a_plus = df / (1-c-(df/df2))\n",
    "\n",
    "            F = M / a_plus\n",
    "\n",
    "        else:\n",
    "\n",
    "            a_minus = df2 / (1-c+(2/df2))\n",
    "\n",
    "            F = (df2*M) / (df*(a_minus-M))\n",
    "\n",
    "        print('M = {}'.format(M))\n",
    "        print('c = {}'.format(c))\n",
    "        print('c2 = {}'.format(c2))\n",
    "        print('-------------------')\n",
    "        print('df = {}'.format(df))\n",
    "        print('df2 = {}'.format(df2))\n",
    "        print('-------------------')\n",
    "        print('F = {}'.format(F)) \n",
    "\n",
    "def plot_mean_and_CI(mean, lb, ub, color_mean=None, color_shading=None):\n",
    "        # plot the shaded range of the confidence intervals\n",
    "        plt.fill_between(range(mean.shape[0]), ub, lb,\n",
    "                         color=color_shading, alpha=.5)\n",
    "        # plot the mean on top\n",
    "        plt.plot(mean, color_mean)\n",
    "\n",
    "\n",
    "feature_dict = {i:label for i,label in zip(\n",
    "                range(14),\n",
    "                  ('Counter',\n",
    "                  'Image',\n",
    "                  'class label',\n",
    "                  'Myr',\n",
    "                  'Viewpoint',\n",
    "                '# Bulges',\n",
    "                   'Sep',\n",
    "                   'Flux Ratio',\n",
    "                  'Gini',\n",
    "                  'M20',\n",
    "                  'Concentration (C)',\n",
    "                  'Asymmetry (A)',\n",
    "                  'Clumpiness (S)',\n",
    "                  'Sersic N',\n",
    "                  'Shape Asymmetry (A_S)'))}\n",
    "\n",
    "#Counter\tImage\tMerger (0 = no, 1 = yes)\tMyr\tViewpoint\tGini\tM20\tC\tA\tS\tSersic n\n",
    "'''view=0\n",
    "df = pd.io.parsers.read_table(\n",
    "    filepath_or_buffer='PCA_img_0.txt',\n",
    "    header=[0],\n",
    "    sep='\\t', skiprows=14*view,nrows=14\n",
    "    )#,skiprows=10,nrows=10'''\n",
    "\n",
    "\n",
    "#list_runs=['fg3_m_12','fg1_m_13']\n",
    "list_runs=[ 'fg3_m15', 'fg3_m12', 'fg1_m13']#,'fg3_m12','all','fg1_m13']#,'fg3_m12','fg3_m12_nolate', 'fg3_m12_noiso']#'fg1_m13','fg3_m15']#,'fg1_m13']\n",
    "\n",
    "for i in range(len(list_runs)):\n",
    "   \n",
    "    add_on=list_runs[i]\n",
    "    print('run', add_on)\n",
    "    \n",
    "\n",
    "    run=list_runs[i]\n",
    "    \n",
    "    df = pd.io.parsers.read_table(\n",
    "        filepath_or_buffer='LDA_img_ratio_statmorph_'+str(run)+'.txt',#'_view_all.txt',#filepath_or_buffer='LDA_img_ratio_'+str(run)+'_early_late_all_things.txt',#'_view_all.txt',\n",
    "        header=[0],\n",
    "        sep='\\t'\n",
    "        )#,skiprows=10,nrows=10\n",
    "        ##filepath_or_buffer='LDA_img_ratio_statmorph_'+str(run)+'.txt',#'_view_all.txt',\n",
    "          \n",
    "    df.columns = [l for i,l in sorted(feature_dict.items())] + ['Shape Asymmetry']\n",
    "    \n",
    "    df.dropna(how=\"all\", inplace=True) # to drop the empty line at file-end\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    '''index_list=[]\n",
    "    for j in range(len(df)):\n",
    "        if df[['class label']].values[j][0]==0:\n",
    "            index_list.append(j)\n",
    "            \n",
    "    df.drop(df.index[index_list], inplace=True)'''\n",
    "    \n",
    "    \n",
    "    \n",
    "    for j in range(len(df)):\n",
    "        if add_on=='fg3_m15':\n",
    "            if df[['Myr']].values[j][0]<60 and df[['Sep']].values[j][0]==0.0 and df[['# Bulges']].values[j][0]==1:#df[['Myr']].values[i][0]\n",
    "            \n",
    "                df.set_value(j,'class label',0.0)\n",
    "        else:\n",
    "            if df[['Myr']].values[j][0]<40 and df[['Sep']].values[j][0]==0.0 and df[['# Bulges']].values[j][0]==1:#df[['Myr']].values[i][0]\n",
    "            #was < 40\n",
    "\n",
    "            #I use this part to check if there is any separation at these points in time\n",
    "            #Or if there are more than two bulges\n",
    "            #print(df[['class label','Myr','Viewpoint','# Bulges', 'Sep']].values[j])\n",
    "\n",
    "            #Then, you can optionally change the class values of all of these viewpoints\n",
    "\n",
    "            #.set_value(index, col, value, \n",
    "                df.set_value(j,'class label',0.0)\n",
    "    \n",
    "    df.dropna(inplace=True) # to drop the empty line at file-end\n",
    "    #print(df)\n",
    "    \n",
    "\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "    X = df[['Gini','M20','Concentration (C)', 'Asymmetry (A)', 'Sersic N', 'Shape Asymmetry']].values\n",
    "    #'Clumpiness (S)',\n",
    "    y = df['class label'].values\n",
    "    print('~~~~Means PRE standardization~~~~')\n",
    "    \n",
    "    X_nonmerg=[]\n",
    "    X_merg=[]\n",
    "    \n",
    "    for l in range(len(y)):\n",
    "        if y[l]==0:\n",
    "            X_nonmerg.append(X[l])\n",
    "        else:\n",
    "            X_merg.append(X[l])\n",
    "            \n",
    "    \n",
    "    \n",
    "    from sklearn import preprocessing\n",
    "\n",
    "    \n",
    "\n",
    "    std_scale = preprocessing.StandardScaler().fit(X)\n",
    "    X = std_scale.transform(X)\n",
    "    n_params=6\n",
    "\n",
    "\n",
    "    enc = LabelEncoder()\n",
    "    label_encoder = enc.fit(y)\n",
    "    y = label_encoder.transform(y) + 1\n",
    "\n",
    "\n",
    "    label_dict = {1: 'NonMerger', 2: 'Merger'}\n",
    "    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "    # LDA\n",
    "    sklearn_lda = LDA(priors=[0.9,0.1], store_covariance=True)#store_covariance=False\n",
    "    \n",
    "    \n",
    "    \n",
    "    X_lda_sklearn = sklearn_lda.fit_transform(X, y)\n",
    "    #print('predictsions', X_lda_sklearn)\n",
    "    dec = sklearn_lda.score(X,y)\n",
    "    prob = sklearn_lda.predict_proba(X)\n",
    "    \n",
    "    coef = sklearn_lda.coef_\n",
    "    inter = sklearn_lda.intercept_\n",
    "    class_label = sklearn_lda.classes_\n",
    "    cov = sklearn_lda.covariance_\n",
    "    \n",
    "    #print('covariance LDA', cov)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    pred =sklearn_lda.predict(X)\n",
    "    #print('confusion matrix', confusion_matrix(pred,y))\n",
    "    \n",
    "    \n",
    "    #cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    '''from sklearn.model_selection import KFold\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    kf = KFold(n_splits=10, random_state=True, shuffle=True)#len(X))\n",
    "    #kf = StratifiedKFold(n_splits=10, random_state=True, shuffle=True)#len(X))\n",
    "    \n",
    "    \n",
    "    kf.get_n_splits(X)\n",
    "   \n",
    "    \n",
    "    \n",
    "    coef_list=[]\n",
    "    inter_list=[]\n",
    "    confusion_master=[]\n",
    "    count=0\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        \n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        \n",
    "        sklearn_lda = LDA(priors=[0.9,0.1], store_covariance=True)#store_covariance=False\n",
    "    \n",
    "    \n",
    "    \n",
    "        X_lda_sklearn = sklearn_lda.fit_transform(X_train, y_train)\n",
    "        coef = sklearn_lda.coef_\n",
    "        inter = sklearn_lda.intercept_\n",
    "        #print('coef kfold', coef)\n",
    "        coef_list.append(coef)\n",
    "        inter_list.append(inter)\n",
    "        \n",
    "        \n",
    "        \n",
    "        pred =sklearn_lda.predict(X_test)\n",
    "        \n",
    "        \n",
    "        \n",
    "        confusion_master.append(confusion_matrix(pred,y_test))\n",
    "        \n",
    "        \n",
    "        count+=1\n",
    "        \n",
    "        \n",
    "    print('run', add_on)\n",
    "    print('~~~~~K-fold validation Means ~~~~~~')\n",
    "    print(np.mean(coef_list, axis=0))\n",
    "    print(np.mean(inter_list, axis=0))\n",
    "    print('~~~~~K-fold validation STD ~~~~~~')\n",
    "    print(np.std(coef_list, axis=0))\n",
    "    print(np.std(inter_list, axis=0))'''\n",
    "    \n",
    "    \n",
    "    '''Run keras deep learning model'''\n",
    "    # Import `Sequential` from `keras.models`\n",
    "    from keras.models import Sequential\n",
    "\n",
    "    # Import `Dense` from `keras.layers`\n",
    "    from keras.layers import Dense\n",
    "\n",
    "    \n",
    "    \n",
    "    from keras.layers import Convolution2D, MaxPooling2D\n",
    "    # Initialize the constructor\n",
    "    model = Sequential()\n",
    "    # Add an input layer \n",
    "    model.add(Dense(12, activation='relu', input_shape=(n_params,)))\n",
    "    # Add one hidden layer \n",
    "    model.add(Dense(8, activation='relu'))\n",
    "\n",
    "    # Add an output layer \n",
    "    model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "        # 8. Compile model\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    from keras import utils as np_utils\n",
    "    print(list(y_train-1), np.shape(list(y_train-1)))\n",
    "    y_train = np_utils.to_categorical(list(y_train-1), 2)\n",
    "    y_test = np_utils.to_categorical(list(y_test-1), 2)\n",
    "    print(y_train)\n",
    "        \n",
    "    \n",
    "    \n",
    "    model.fit(X_train, y_train,epochs=20, batch_size=1, verbose=1)\n",
    "    score = model.evaluate(X_test, y_test, verbose=1)\n",
    "    print('score', score)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    from sklearn.model_selection import KFold\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    kf = StratifiedKFold(n_splits=10, random_state=True, shuffle=True)#len(X))\n",
    "    \n",
    "    \n",
    "    kf.get_n_splits(X, y)\n",
    "   \n",
    "    \n",
    "    \n",
    "    coef_list=[]\n",
    "    inter_list=[]\n",
    "    confusion_master=[]\n",
    "    y_test_master=[]\n",
    "    pred_master=[]\n",
    "    count=0\n",
    "    for train_index, test_index in kf.split(X, y):\n",
    "        \n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        \n",
    "        sklearn_lda = LDA(priors=[0.9,0.1], store_covariance=True)#store_covariance=False\n",
    "    \n",
    "    \n",
    "    \n",
    "        X_lda_sklearn = sklearn_lda.fit_transform(X_train, y_train)\n",
    "        coef = sklearn_lda.coef_\n",
    "        inter = sklearn_lda.intercept_\n",
    "        #print('coef kfold', coef)\n",
    "        coef_list.append(coef)\n",
    "        inter_list.append(inter)\n",
    "        \n",
    "        \n",
    "        \n",
    "        pred =sklearn_lda.predict(X_test)\n",
    "        \n",
    "        '''plt.clf()\n",
    "        fig=plt.figure()#figsize=(6,6)\n",
    "        plot_confusion_matrix(confusion_matrix(pred,y_test)/np.sum(confusion_matrix(pred,y_test)), sklearn_lda.classes_, title='Normalized Confusion Matrix')\n",
    "        plt.savefig('../MaNGA_Papers/Paper_I/Confusion_matrix_'+str(run)+'_'+str(count)+'.pdf')\n",
    "        #This is from this website: http://www.science.smith.edu/~jcrouser/SDS293/labs/lab5-py.html\n",
    "        plt.clf()'''\n",
    "        \n",
    "        confusion_master.append(confusion_matrix(pred,y_test))\n",
    "        pred_master.append(pred)\n",
    "        y_test_master.append(y_test)\n",
    "        \n",
    "        count+=1\n",
    "        \n",
    "        \n",
    "    print('~~~~~Stratified K-fold validation means  ~~~~~~')\n",
    "    print(np.mean(coef_list, axis=0))\n",
    "    print(np.mean(inter_list, axis=0))\n",
    "    print('~~~~~Stratified K-fold validation STD ~~~~~~')\n",
    "    print(np.std(coef_list, axis=0))\n",
    "    print(np.std(inter_list, axis=0))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print('~~~~~Comparing to last run~~~~~')\n",
    "    print(coef)\n",
    "    print(inter)\n",
    "    \n",
    "    print('~~~~~Master Confusion~~~~~')\n",
    "    '''print(confusion_master)\n",
    "    print(np.shape(confusion_master))'''\n",
    "    print(np.mean(confusion_master, axis=0))\n",
    "    \n",
    "    \n",
    "    \n",
    "    plt.clf()\n",
    "    fig=plt.figure()#figsize=(6,6)\n",
    "    plot_confusion_matrix(np.mean(confusion_master,axis=0)/np.sum(np.mean(confusion_master,axis=0)), sklearn_lda.classes_, title='Normalized Confusion Matrix')\n",
    "    plt.savefig('../MaNGA_Papers/Paper_I/Confusion_matrix_'+str(run)+'.pdf')\n",
    "    #This is from this website: http://www.science.smith.edu/~jcrouser/SDS293/labs/lab5-py.html\n",
    "    plt.clf()\n",
    "    \n",
    "    \n",
    "    '''Okay try repeating with different values for k'''\n",
    "    '''k=np.linspace(2,len(X)-1, len(X)-1)\n",
    "    #np.linspace(1,208,208)\n",
    "    #k=[3,4,5,6,8,10,20]\n",
    "    avg_error_0=[]\n",
    "    avg_mean_0=[]\n",
    "    avg_dec_0=[]\n",
    "    index=0\n",
    "    for p in range(len(k)):\n",
    "        split=k[p]+1\n",
    "        print('k split', int(split))\n",
    "        kf = KFold(n_splits=int(split))\n",
    "        kf.get_n_splits(X)\n",
    "\n",
    "\n",
    "        coef_list=[]\n",
    "        \n",
    "        for train_index, test_index in kf.split(X):\n",
    "            #print('TRAIN:', train_index, 'TEST:', test_index)\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            \n",
    "            sklearn_lda = LDA(priors=[0.9,0.1])#store_covariance=False\n",
    "            X_lda_sklearn = sklearn_lda.fit_transform(X_train, y_train)\n",
    "            coef = sklearn_lda.coef_\n",
    "            coef_list.append(coef)\n",
    "            dec = sklearn_lda.score(X_test,y_test)\n",
    "        avg_dec_0.append(np.mean(dec))\n",
    "        avg_mean_0.append(np.mean(coef_list, axis=0)[0]) \n",
    "        avg_error_0.append(np.std(coef_list, axis=0)[0])\n",
    "    mean_list=[]\n",
    "    std_list=[]\n",
    "    for o in range(np.shape(avg_mean_0)[0]):\n",
    "        mean_list.append(avg_mean_0[o][index])\n",
    "        std_list.append(avg_error_0[o][index])\n",
    "    mean_list=np.array(mean_list)\n",
    "    std_list=np.array(std_list)\n",
    "    plt.clf()\n",
    "    plt.plot(k, mean_list, color='red')\n",
    "    plt.fill_between(k, mean_list-std_list, mean_list+std_list, alpha=0.5, color='pink')\n",
    "    #plot_mean_and_CI(avg_mean_0, avg_error_0, avg_error_0)\n",
    "    plt.xlabel(r'# of k folds')\n",
    "    plt.ylabel(r'LD1: $Gini$ Coefficient')\n",
    "    plt.axvline(x=10, color='red', ls='--')\n",
    "    plt.savefig('../MaNGA_Papers/Paper_I/kfold_'+str(index)+'_'+str(run)+'.pdf')\n",
    "    \n",
    "    plt.clf()\n",
    "    plt.plot(k, avg_dec_0, color='red')\n",
    "    \n",
    "    plt.xlabel(r'# of k folds')\n",
    "    plt.ylabel(r'% Accuracy')\n",
    "    plt.axvline(x=10, color='red', ls='--')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../MaNGA_Papers/Paper_I/kfold_acc_'+str(run)+'.pdf')'''\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #coef_ : array, shape (n_features,) or (n_classes, n_features)\n",
    "\n",
    "    #intercept_ : array, shape (n_features,)\n",
    "\n",
    "    #Intercept term.\n",
    "\n",
    "    #covariance_ : \n",
    "    #print('coeff weights', coef)\n",
    "    #print('covariance', cov)\n",
    "    '''for l in range(len(coef[0])):\n",
    "        print('weight', coef[0][l], 'cov', cov[l][l], 'sigma', 1/np.sqrt(cov[l][l]))\n",
    "    '''\n",
    "    \n",
    "    \n",
    "   \n",
    "    print('mean accuracy',dec)#mean accuracy on the given test data and labels.\n",
    "    \n",
    "    print(inter)\n",
    "    \n",
    "    \n",
    "    from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "    \n",
    "    # QDA\n",
    "    sklearn_qda = QDA(priors=[0.9,0.1])\n",
    "    X_qda_sklearn = sklearn_qda.fit(X, y, store_covariances=True)\n",
    "    dec_qda = sklearn_qda.score(X,y)\n",
    "    cov_q = sklearn_qda.covariances_\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    pred_q =sklearn_qda.predict(X_test)\n",
    "    print('confusion matrix QDA', confusion_matrix(pred_q,y_test))\n",
    "    print(classification_report(y_test, pred_q, digits=3))\n",
    "    \n",
    "    \n",
    "    plt.clf()\n",
    "    fig=plt.figure()#figsize=(6,6)\n",
    "    plot_confusion_matrix(confusion_matrix(pred_q,y_test)/np.sum(confusion_matrix(pred_q,y_test)), sklearn_qda.classes_, title='Normalized confusion matrix')\n",
    "    plt.savefig('../MaNGA_Papers/Paper_I/Confusion_matrix_qda_'+str(run)+'.pdf')\n",
    "    #This is from this website: http://www.science.smith.edu/~jcrouser/SDS293/labs/lab5-py.html\n",
    "    plt.clf()\n",
    "    \n",
    "    \n",
    "    \n",
    "    '''Redo X_lda to make plots'''\n",
    "    X_lda_sklearn = sklearn_lda.fit_transform(X, y)\n",
    "    #coef = sklearn_qda.coef_\n",
    "    #inter = sklearn_qda.intercept_\n",
    "    print(dec_qda)#mean accuracy on the given test data and labels.\n",
    "\n",
    "    '''Make a histogram'''\n",
    "    from scipy import stats\n",
    "    import seaborn as sns\n",
    "    plt.clf()\n",
    "    fig = plt.figure(figsize=(18,6))\n",
    "    ax = fig.add_subplot(111)\n",
    "    X_lda_1=[]\n",
    "    X_lda_2=[]\n",
    "    for j in range(len(X_lda_sklearn)):\n",
    "        if y[j] ==1:\n",
    "            X_lda_1.append(X_lda_sklearn[j][0])\n",
    "        else:\n",
    "            X_lda_2.append(X_lda_sklearn[j][0])\n",
    "    input_hist=X_lda_sklearn\n",
    "    \n",
    "    ax.hist(X_lda_1, label='Nonmerger',  color=sns.xkcd_rgb[\"sky blue\"],alpha = 0.85)\n",
    "    ax.hist(X_lda_2, label='Merger',  color=sns.xkcd_rgb[\"salmon\"],alpha = 0.85)\n",
    "\n",
    "    '''for label,col in zip(range(1,4),  ('blue', 'red')):\n",
    "        input_hist=X_lda_sklearn\n",
    "        input_all=X_lda_sklearn\n",
    "        ax.hist(input_hist,\n",
    "                       color=col,\n",
    "                       label='class %s' %label_dict[label],\n",
    "                       alpha=0.5,)#bins=bins,\n",
    "        xt = plt.xticks()[0]  \n",
    "        xmin, xmax = -0.1,0.7#min(xt), max(xt)  \n",
    "        lnspc = np.linspace(xmin, xmax, len(input_hist))\n",
    "\n",
    "        # lets try the normal distribution first\n",
    "        m, s = stats.norm.fit(input_hist) # get mean and standard deviation  \n",
    "        pdf_g = stats.norm.pdf(lnspc, m, s) # now get theoretical values in our interval  \n",
    "        #ax.plot(lnspc, pdf_g,  color=col) # plot it\n",
    "\n",
    "\n",
    "\n",
    "    ylims = ax.get_ylim()\n",
    "\n",
    "    # plot annotation\n",
    "    leg = ax.legend(loc='upper right', fancybox=True, fontsize=8)\n",
    "    leg.get_frame().set_alpha(0.5)\n",
    "    ax.set_ylim([0, max(ylims)+2])'''\n",
    "\n",
    "    ax.set_xlabel('LD1', size=20)\n",
    "    #ax.set_title('Histogram #%s' %str(cnt+1), size=20)\n",
    "\n",
    "    # hide axis ticks\n",
    "    ax.tick_params(axis=\"both\", which=\"both\", bottom=\"off\", top=\"off\",  \n",
    "            labelbottom=\"on\", left=\"off\", right=\"off\", labelleft=\"on\", labelsize=20)\n",
    "\n",
    "    # remove axis spines\n",
    "    ax.spines[\"top\"].set_visible(False)  \n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines[\"bottom\"].set_visible(False)\n",
    "    ax.spines[\"left\"].set_visible(False)    \n",
    "\n",
    "    ax.set_ylabel('Count', size=20)\n",
    "    \n",
    "    \n",
    "    plt.legend(loc=\"upper right\", fontsize=20)\n",
    "    #fig.tight_layout() \n",
    "    #plt.annotate(str(add_on), xy=(0.02,0.95),xycoords='axes fraction', size=20)\n",
    "    #plt.annotate('Mean Accuracy = '+str(dec), xy=(0.02,0.9),xycoords='axes fraction', size=20)\n",
    "    #frame1 = plt.gca()\n",
    "    \n",
    "    \n",
    "    #plt.savefig('../MaNGA_Papers/Paper_I/Marginalized_img_'+str(run)+'.pdf')\n",
    "    #plt.clf()\n",
    "    \n",
    "    '''Also, making those mountain plots for the imaging runs'''\n",
    "    \n",
    "    \n",
    "    \n",
    "    '''Now measure LD1 for every row and then plot that'''\n",
    "    import seaborn as sns\n",
    "    \n",
    "\n",
    "    n_params=7\n",
    "\n",
    "\n",
    "\n",
    "    #coef is how you get the eigvecs (doesn't matter what slope offset is)\n",
    "    #print('real eigvecs',(eigvec_sc.real))\n",
    "    #print(len(X_lda[:,0].real[y==2]))#[y == label]\n",
    "    xs=[]\n",
    "    LDA1=[]\n",
    "    if run=='fg3_m15':\n",
    "        myr=[5,30,60,100,120,150,180,200,210,240,270,300,320,340,360,400,420]\n",
    "        myr_non=myr#[5,100,200]\n",
    "    if run=='fg3_m_12':\n",
    "        myr=[170,180,185,190,195,205,210,220,225,230,240,250,260]\n",
    "        myr_non=[5,200]\n",
    "    if run=='fg3_m12':\n",
    "        myr=[5,10,20,30,40,60,80,100,120,140,160,170,180,185,190,195,200,205,210,220,225,230,240,250,260]\n",
    "        myr_non=myr\n",
    "    if run=='fg1_m13':\n",
    "        myr=[10,40,50,60,70,90,100,120,130,140,170,180,185,190,195,200,205,210,215,220,225,230,235,240,250,260,270,280,290,300,310,320,330,340,350]\n",
    "        myr_non=myr_non\n",
    "    if run=='fg1_m_13':\n",
    "        myr=[40,195,210,215,220,225,230,235,240,250,260,270,280,290,300,310,320,330,340,350]\n",
    "        myr_non=[5,200]\n",
    "    if run=='all':\n",
    "        myr=[5,10,20,30,40,50,60,70,80,90,100,120,130,140,160,170,180,185,190,195,200,205,210,215,220,225,230,235,240,250,260,270,280,290,300,310,320,330,340,350, 360, 400, 420]\n",
    "        myr_non=myr\n",
    "    my_lists = {key:[] for key in myr}\n",
    "    my_lists_non = []\n",
    "    my_lists_merg = []\n",
    "    separations = {key:[] for key in myr}\n",
    "\n",
    "    print(df[['class label']])\n",
    "    #STOP\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        if df[['class label']].values[i]==0:\n",
    "            my_lists_non.append(X_lda_sklearn[i][0])\n",
    "            continue\n",
    "        if df[['class label']].values[i]==1:\n",
    "            my_lists_merg.append(X_lda_sklearn[i][0])\n",
    "        \n",
    "        my_lists[df[['Myr']].values[i][0]].append(X_lda_sklearn[i][0])\n",
    "        \n",
    "        separations[df[['Myr']].values[i][0]].append(df[['Sep']].values[i][0])\n",
    "        L=X_lda_sklearn[i][0]\n",
    "        #df[['Gini']].values[i][0]*coef[0][0]+df[['M20']].values[i][0]*coef[0][1]+df[['Concentration (C)']].values[i][0]*coef[0][2]+df[['Asymmetry (A)']].values[i][0]*coef[0][3]+df[['Clumpiness (S)']].values[i][0]*coef[0][4]+df[['Sersic N']].values[i][0]*coef[0][5]+df[['Shape Asymmetry']].values[i][0]*coef[0][6]\n",
    "        LDA1.append(L)\n",
    "        xs.append(df[['Myr']].values[i][0])\n",
    "\n",
    "    #print(mean(my_lists[180]))\n",
    "    mean_non=(np.mean(my_lists_merg)+np.mean(my_lists_non))/2\n",
    "    \n",
    "    \n",
    "    if run=='fg1_m13':\n",
    "        plt.annotate('q0.333_fg0.1', xy=(0.01,0.92), xycoords='axes fraction', size=20)\n",
    "    #if run=='fg3_m12':\n",
    "        #plt.annotate('q0.5_fg0.3', xy=(0.01,0.92), xycoords='axes fraction', size=20)\n",
    "    if run=='fg3_m15':\n",
    "        plt.annotate('q0.2_fg0.3_BT0.2', xy=(0.01,0.92), xycoords='axes fraction', size=20)\n",
    "    if run=='all':\n",
    "        plt.annotate('All', xy=(0.01,0.92), xycoords='axes fraction', size=20)\n",
    "    #plt.axvline(x=mean_non, color='black')\n",
    "    #plt.annotate('Mean Accuracy = '+str(round(dec,2)), xy=(0.3,0.92), xycoords='axes fraction', size=20)\n",
    "    plt.savefig('../MaNGA_Papers/Paper_I/Hist_statmorph_'+str(run)+'.pdf')\n",
    "    plt.clf()\n",
    "    \n",
    "    \n",
    "    \n",
    "    '''plt.clf()\n",
    "    fig = plt.figure(figsize=(18,6))\n",
    "    ax = fig.add_subplot(111)\n",
    "    X_qda_1=[]\n",
    "    X_qda_2=[]\n",
    "    for j in range(len(X_qda_sklearn)):\n",
    "        if y[j] ==1:\n",
    "            X_qda_1.append(X_qda_sklearn[j][0])\n",
    "        else:\n",
    "            X_qda_2.append(X_qda_sklearn[j][0])\n",
    "    input_hist=X_lda_sklearn\n",
    "    \n",
    "    ax.hist(X_qda_1, label='Nonmerger',  color=sns.xkcd_rgb[\"sky blue\"],alpha = 0.85)\n",
    "    ax.hist(X_qda_2, label='Merger',  color=sns.xkcd_rgb[\"salmon\"],alpha = 0.85)\n",
    "\n",
    "   \n",
    "\n",
    "    ax.set_xlabel('QD1', size=20)\n",
    "    #ax.set_title('Histogram #%s' %str(cnt+1), size=20)\n",
    "\n",
    "    # hide axis ticks\n",
    "    ax.tick_params(axis=\"both\", which=\"both\", bottom=\"off\", top=\"off\",  \n",
    "            labelbottom=\"on\", left=\"off\", right=\"off\", labelleft=\"on\", labelsize=20)\n",
    "\n",
    "    # remove axis spines\n",
    "    ax.spines[\"top\"].set_visible(False)  \n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines[\"bottom\"].set_visible(False)\n",
    "    ax.spines[\"left\"].set_visible(False)    \n",
    "\n",
    "    ax.set_ylabel('Count', size=20)\n",
    "    \n",
    "    \n",
    "    plt.legend(loc=\"upper right\", fontsize=20)\n",
    " \n",
    "    my_lists_non_qda = []\n",
    "    \n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        if df[['class label']].values[i]==0 or df[['Myr']].values[i][0]==200:\n",
    "            my_lists_non.append(X_qda_sklearn[i][0])\n",
    "            continue\n",
    "        \n",
    "\n",
    "    #print(mean(my_lists[180]))\n",
    "    mean_non=np.mean(my_lists_non)+np.std(my_lists_non)\n",
    "    \n",
    "    \n",
    "    if run=='fg1_m13':\n",
    "        plt.annotate('q0.333_fg0.1', xy=(0.01,0.92), xycoords='axes fraction', size=20)\n",
    "    if run=='fg3_m12':\n",
    "        plt.annotate('q0.5_fg0.3', xy=(0.01,0.92), xycoords='axes fraction', size=20)\n",
    "    if run=='fg3_m15':\n",
    "        plt.annotate('q0.2_fg0.3', xy=(0.01,0.92), xycoords='axes fraction', size=20)\n",
    "    plt.axvline(x=mean_non, color='black')\n",
    "    plt.savefig('../MaNGA_Papers/Paper_I/Hist_statmorph_qda_'+str(run)+'.pdf')'''\n",
    "    \n",
    "\n",
    "    \n",
    "    means=[]\n",
    "    std=[]\n",
    "    separation_value=[]\n",
    "    myr_here=[]\n",
    "    for i in range(len(myr)):\n",
    "        if math.isnan(np.mean(my_lists[myr[i]])):\n",
    "            continue\n",
    "        else:\n",
    "            means.append(np.mean(my_lists[myr[i]]))\n",
    "            std.append(np.std(my_lists[myr[i]]))\n",
    "            separation_value.append(np.mean(separations[myr[i]]))\n",
    "            myr_here.append(myr[i])\n",
    "    print('myr',myr)\n",
    "    print('separations',separation_value)\n",
    "\n",
    "    means=np.array(means)\n",
    "    std=np.array(std)\n",
    "    myr=np.array(myr_here)\n",
    "    \n",
    "    print('means',means)\n",
    "    \n",
    "    if run=='fg1_m13':\n",
    "        '''Try to replot with a dimensionless y axis with just means and std'''\n",
    "        rescale_y_mean=0#-1.7583e-01\n",
    "        rescale_y_pm=7.13475416061/2\n",
    "\n",
    "\n",
    "        new_means=np.array([(x-rescale_y_mean) for x in means])\n",
    "\n",
    "        plt.clf()\n",
    "\n",
    "        plt.plot(myr/100, new_means, color=sns.xkcd_rgb[\"amber\"])\n",
    "        plt.fill_between(myr/100, (new_means-std), (new_means+std),alpha=.5, color=sns.xkcd_rgb[\"amber\"])\n",
    "        plt.xlabel(r'Merger Timeline [Gyr]', size=15)\n",
    "        plt.ylabel(r'Detection Sensitivity (LD1)', size=15)\n",
    "        #plt.axvline(x=220/100, color='black', ls='--')\n",
    "        plt.axvline(x=215/100, color='black', ls='--')\n",
    "        plt.axvline(x=280/100, color='black', ls='--')\n",
    "\n",
    "        '''width and height 3.17421463155 0.87713041243\n",
    "        pos [ -1.6779e+00   2.6963e-16]\n",
    "        width and height 5.56843021209 2.94988106646\n",
    "        pos [  2.7636e-01   1.2800e-16]'''\n",
    "\n",
    "\n",
    "        '''fg1_m_13:\n",
    "        width and height 1.98151143695 1.33924739273\n",
    "        pos [  1.4067e+00   6.6911e-16]\n",
    "        width and height 7.13475416061 2.66110273731\n",
    "        pos [ -1.7583e-01  -1.3878e-16]'''\n",
    "\n",
    "        plt.axhline(y=mean_non, color='black')\n",
    "        #plt.annotate(r'$\\mu_{\\mathrm{Merger}}$', xy=(0.02,0.53), xycoords='axes fraction', size=15)\n",
    "        #plt.title(str(run))\n",
    "        plt.annotate('q0.333_fg0.1', xy=(0.02,1.02), xycoords='axes fraction', size=20)\n",
    "        plt.annotate('Early', xy=(0.25,0.95), xycoords='axes fraction', size=9)\n",
    "        plt.annotate('Late', xy=(0.63,0.95), xycoords='axes fraction',size=9)\n",
    "        plt.annotate('Post Coalescence', xy=(0.76,1.02), xycoords='axes fraction', size=9)\n",
    "\n",
    "    if run=='fg1_m_13':\n",
    "        '''Try to replot with a dimensionless y axis with just means and std'''\n",
    "        rescale_y_mean=0#-1.7583e-01\n",
    "        rescale_y_pm=7.13475416061/2\n",
    "\n",
    "\n",
    "        new_means=np.array([(x-rescale_y_mean) for x in means])\n",
    "\n",
    "        plt.clf()\n",
    "\n",
    "        plt.plot(myr/100, new_means, color=sns.xkcd_rgb[\"amber\"])\n",
    "        plt.fill_between(myr/100, (new_means-std), (new_means+std),alpha=.5, color=sns.xkcd_rgb[\"amber\"])\n",
    "        plt.xlabel(r'Merger Timeline [Gyr]', size=15)\n",
    "        plt.ylabel(r'Detection Sensitivity (LD1)', size=15)\n",
    "        #plt.axvline(x=220/100, color='black', ls='--')\n",
    "        plt.axvline(x=215/100, color='black', ls='--')\n",
    "        plt.axvline(x=280/100, color='black', ls='--')\n",
    "\n",
    "        '''width and height 3.17421463155 0.87713041243\n",
    "        pos [ -1.6779e+00   2.6963e-16]\n",
    "        width and height 5.56843021209 2.94988106646\n",
    "        pos [  2.7636e-01   1.2800e-16]'''\n",
    "\n",
    "\n",
    "        '''fg1_m_13:\n",
    "        width and height 1.98151143695 1.33924739273\n",
    "        pos [  1.4067e+00   6.6911e-16]\n",
    "        width and height 7.13475416061 2.66110273731\n",
    "        pos [ -1.7583e-01  -1.3878e-16]'''\n",
    "\n",
    "        ys_LD1=np.array([-1.7583e-01 for x in myr])\n",
    "        #plt.plot(myr/100,ys_LD1)\n",
    "        #plt.fill_between(myr/100, ys_LD1-0.588/2, ys_LD1+0.588/2,alpha=.5)\n",
    "        plt.axhline(y=mean_non, color='black')\n",
    "        #plt.annotate(r'$\\mu_{\\mathrm{Merger}}$', xy=(0.02,0.53), xycoords='axes fraction', size=15)\n",
    "        #plt.title(str(run))\n",
    "        plt.annotate('FG1M13', xy=(0.02,1.02), xycoords='axes fraction', size=20)\n",
    "        plt.annotate('Early', xy=(0.25,0.95), xycoords='axes fraction', size=9)\n",
    "        plt.annotate('Late', xy=(0.63,0.95), xycoords='axes fraction',size=9)\n",
    "        plt.annotate('Post Coalescence', xy=(0.76,1.02), xycoords='axes fraction', size=9)\n",
    "    if run=='fg3_m_12':\n",
    "        '''Try to replot with a dimensionless y axis with just means and std'''\n",
    "        rescale_y_mean=0#2.7636e-01\n",
    "        rescale_y_pm=5.5684302120/2\n",
    "\n",
    "\n",
    "        new_means=np.array([(x-rescale_y_mean) for x in means])\n",
    "\n",
    "        plt.clf()\n",
    "\n",
    "        plt.plot(myr/100, new_means, color='red')\n",
    "        plt.fill_between(myr/100, (new_means-std), (new_means+std),alpha=.5, color='red')\n",
    "        plt.xlabel(r'Merger Timeline [Gyr]', size=15)\n",
    "        plt.ylabel(r'Detection Sensitivity (LD1)', size=15)\n",
    "        plt.axvline(x=220/100, color='black', ls='--')\n",
    "        plt.axvline(x=180/100, color='black', ls='--')\n",
    "\n",
    "\n",
    "\n",
    "        '''width and height 3.17421463155 0.87713041243\n",
    "        pos [ -1.6779e+00   2.6963e-16]\n",
    "        width and height 5.56843021209 2.94988106646\n",
    "        pos [  2.7636e-01   1.2800e-16]'''\n",
    "\n",
    "\n",
    "        #plt.axhline(y=0, color='black')\n",
    "\n",
    "        #plt.annotate(r'$\\mu_{\\mathrm{Merger}}$', xy=(0.15,0.53), xycoords='axes fraction', size=15)\n",
    "        #plt.title(str(run))\n",
    "        plt.axhline(y=mean_non, color='black')\n",
    "        plt.annotate('FG3M12', xy=(0.02,1.02), xycoords='axes fraction', size=20)\n",
    "        plt.annotate('Early', xy=(0.03,0.95), xycoords='axes fraction', size=9)\n",
    "        plt.annotate('Late', xy=(0.3,0.95), xycoords='axes fraction',size=9)\n",
    "        plt.annotate('Post Coalescence', xy=(0.65,0.95), xycoords='axes fraction', size=9)\n",
    "    if run=='fg3_m12':\n",
    "    \n",
    "        '''Try to replot with a dimensionless y axis with just means and std'''\n",
    "        rescale_y_mean=0#2.7636e-01\n",
    "        rescale_y_pm=5.5684302120/2\n",
    "\n",
    "\n",
    "        new_means=np.array([(x-rescale_y_mean) for x in means])\n",
    "\n",
    "        plt.clf()\n",
    "\n",
    "        plt.plot(myr/100, new_means, color='red')\n",
    "        plt.fill_between(myr/100, (new_means-std), (new_means+std),alpha=.5, color='red')\n",
    "        plt.xlabel(r'Merger Timeline [Gyr]', size=15)\n",
    "        plt.ylabel(r'Detection Sensitivity (LD1)', size=15)\n",
    "        plt.axvline(x=220/100, color='black', ls='--')\n",
    "        plt.axvline(x=180/100, color='black', ls='--')\n",
    "\n",
    "\n",
    "\n",
    "        '''width and height 3.17421463155 0.87713041243\n",
    "        pos [ -1.6779e+00   2.6963e-16]\n",
    "        width and height 5.56843021209 2.94988106646\n",
    "        pos [  2.7636e-01   1.2800e-16]'''\n",
    "\n",
    "\n",
    "        #plt.axhline(y=0, color='black')\n",
    "\n",
    "        #plt.annotate(r'$\\mu_{\\mathrm{Merger}}$', xy=(0.15,0.53), xycoords='axes fraction', size=15)\n",
    "        #plt.title(str(run))\n",
    "        plt.axhline(y=mean_non, color='black')\n",
    "        plt.annotate('q0.5_fg0.3', xy=(0.02,1.02), xycoords='axes fraction', size=20)\n",
    "        plt.annotate('Early', xy=(0.63,0.97), xycoords='axes fraction', size=9)\n",
    "        plt.annotate('Late', xy=(0.7,0.97), xycoords='axes fraction',size=9)\n",
    "        plt.annotate('Post Coalescence', xy=(0.85,1.01), xycoords='axes fraction', size=9)\n",
    "    if run=='fg3_m15':\n",
    "    \n",
    "        '''Try to replot with a dimensionless y axis with just means and std'''\n",
    "        rescale_y_mean=0#2.7636e-01\n",
    "        rescale_y_pm=5.5684302120/2\n",
    "\n",
    "\n",
    "        new_means=np.array([(x-rescale_y_mean) for x in means])\n",
    "\n",
    "        plt.clf()\n",
    "\n",
    "        plt.plot(myr/100, new_means, color='blue')\n",
    "        plt.fill_between(myr/100, (new_means-std), (new_means+std),alpha=.5, color='blue')\n",
    "        plt.xlabel(r'Merger Timeline [Gyr]', size=15)\n",
    "        plt.ylabel(r'Detection Sensitivity (LD1)', size=15)\n",
    "        plt.axvline(x=360/100, color='black', ls='--')\n",
    "        plt.axvline(x=180/100, color='black', ls='--')\n",
    "\n",
    "\n",
    "\n",
    "        '''width and height 3.17421463155 0.87713041243\n",
    "        pos [ -1.6779e+00   2.6963e-16]\n",
    "        width and height 5.56843021209 2.94988106646\n",
    "        pos [  2.7636e-01   1.2800e-16]'''\n",
    "\n",
    "\n",
    "        #plt.axhline(y=0, color='black')\n",
    "\n",
    "        #plt.annotate(r'$\\mu_{\\mathrm{Merger}}$', xy=(0.15,0.53), xycoords='axes fraction', size=15)\n",
    "        #plt.title(str(run))\n",
    "        plt.axhline(y=mean_non, color='black')\n",
    "        plt.annotate('q0.2_fg0.3_BT0.2', xy=(0.02,1.02), xycoords='axes fraction', size=20)\n",
    "        #plt.annotate('Early', xy=(0.63,0.97), xycoords='axes fraction', size=9)\n",
    "        plt.annotate('Late', xy=(0.35,0.97), xycoords='axes fraction',size=9)\n",
    "        plt.annotate('Post Coalescence', xy=(0.45,0.97), xycoords='axes fraction', size=9)\n",
    "    if run=='all':\n",
    "    \n",
    "        '''Try to replot with a dimensionless y axis with just means and std'''\n",
    "        rescale_y_mean=0#2.7636e-01\n",
    "        \n",
    "        new_means=np.array([(x-rescale_y_mean) for x in means])\n",
    "\n",
    "        plt.clf()\n",
    "\n",
    "        plt.plot(myr/100, new_means, color='green')\n",
    "        plt.fill_between(myr/100, (new_means-std), (new_means+std),alpha=.5, color='green')\n",
    "        plt.xlabel(r'Merger Timeline [Gyr]', size=15)\n",
    "        plt.ylabel(r'Detection Sensitivity (LD1)', size=15)\n",
    "        plt.axvline(x=270/100, color='black', ls='--')\n",
    "        plt.axvline(x=180/100, color='black', ls='--')\n",
    "\n",
    "\n",
    "\n",
    "        '''width and height 3.17421463155 0.87713041243\n",
    "        pos [ -1.6779e+00   2.6963e-16]\n",
    "        width and height 5.56843021209 2.94988106646\n",
    "        pos [  2.7636e-01   1.2800e-16]'''\n",
    "\n",
    "\n",
    "        #plt.axhline(y=0, color='black')\n",
    "\n",
    "        #plt.annotate(r'$\\mu_{\\mathrm{Merger}}$', xy=(0.15,0.53), xycoords='axes fraction', size=15)\n",
    "        #plt.title(str(run))\n",
    "        plt.axhline(y=mean_non, color='black')\n",
    "        plt.annotate('All', xy=(0.02,1.02), xycoords='axes fraction', size=20)\n",
    "        plt.annotate('Early', xy=(1.5,0.97), size=15)\n",
    "        \n",
    "        plt.annotate('Late', xy=(1.9,0.97), size=15)\n",
    "        plt.annotate('Post Coalescence', xy=(2.75,0.97), size=15)\n",
    "    #plt.ylim([-1,1])\n",
    "    plt.xlim([min(myr)/100,max(myr)/100])\n",
    "    frame1 = plt.gca()\n",
    "    plt.tick_params(axis='both', which='major', labelsize=15)\n",
    "    \n",
    "    #frame1.axes.yaxis.set_ticklabels([])\n",
    "    \n",
    "    plt.savefig('../MaNGA_Papers/Paper_I/Mountain_plot_imaging_statmorph_'+str(run)+'.pdf')\n",
    "\n",
    "    \n",
    "\n",
    "    '''Try to replot with a dimensionless y axis with just means and std'''\n",
    "    \n",
    "    \n",
    "#    savefig('../MaNGA_Papers/Paper_I/Bayesian_Hist_'+str(run)+'.pdf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjQAAAGACAYAAAC6OPj9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nbE+FRkRERGxPhUZERERs77+EUbjjYtSoPgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('done')\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(k, avg_dec_0, color='red')\n",
    "\n",
    "plt.xlabel(r'# of k folds')\n",
    "plt.ylabel(r'% Accuracy')\n",
    "plt.axvline(x=10, color='red', ls='--')\n",
    "plt.xlim([0,25])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/beckynevin/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-5-06bfcc9bd58f>\", line 64, in <module>\n",
      "    sep='\\t'\n",
      "  File \"/Users/beckynevin/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\", line 709, in parser_f\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/beckynevin/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\", line 449, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/beckynevin/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\", line 818, in __init__\n",
      "    self._make_engine(self.engine)\n",
      "  File \"/Users/beckynevin/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\", line 1049, in _make_engine\n",
      "    self._engine = CParserWrapper(self.f, **self.options)\n",
      "  File \"/Users/beckynevin/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\", line 1695, in __init__\n",
      "    self._reader = parsers.TextReader(src, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 402, in pandas._libs.parsers.TextReader.__cinit__\n",
      "  File \"pandas/_libs/parsers.pyx\", line 718, in pandas._libs.parsers.TextReader._setup_parser_source\n",
      "FileNotFoundError: File b'LDA_img_ratio_fg1_m13_early_late_all_things.txt' does not exist\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/beckynevin/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1828, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'FileNotFoundError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/beckynevin/anaconda/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1090, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/Users/beckynevin/anaconda/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/beckynevin/anaconda/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/Users/beckynevin/anaconda/lib/python3.6/inspect.py\", line 1454, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/Users/beckynevin/anaconda/lib/python3.6/inspect.py\", line 1411, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/Users/beckynevin/anaconda/lib/python3.6/inspect.py\", line 666, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/Users/beckynevin/anaconda/lib/python3.6/inspect.py\", line 695, in getmodule\n",
      "    file = getabsfile(object, _filename)\n",
      "  File \"/Users/beckynevin/anaconda/lib/python3.6/inspect.py\", line 679, in getabsfile\n",
      "    return os.path.normcase(os.path.abspath(_filename))\n",
      "  File \"/Users/beckynevin/anaconda/lib/python3.6/posixpath.py\", line 374, in abspath\n",
      "    cwd = os.getcwd()\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'LDA_img_ratio_fg1_m13_early_late_all_things.txt' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "'''\n",
    "A bootstrap method to determine errors by dropping each galaxy individually\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "~~~\n",
    "Now just for the imaging part of it!\n",
    "~~~\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "feature_dict = {i:label for i,label in zip(\n",
    "                range(14),\n",
    "                  ('Counter',\n",
    "                  'Image',\n",
    "                  'class label',\n",
    "                  'Myr',\n",
    "                  'Viewpoint',\n",
    "                '# Bulges',\n",
    "                   'Sep',\n",
    "                   'Flux Ratio',\n",
    "                  'Gini',\n",
    "                  'M20',\n",
    "                  'Concentration (C)',\n",
    "                  'Asymmetry (A)',\n",
    "                  'Clumpiness (S)',\n",
    "                  'Sersic N',\n",
    "                  'Shape Asymmetry (A_S)'))}\n",
    "\n",
    "#Counter\tImage\tMerger (0 = no, 1 = yes)\tMyr\tViewpoint\tGini\tM20\tC\tA\tS\tSersic n\n",
    "'''view=0\n",
    "df = pd.io.parsers.read_table(\n",
    "    filepath_or_buffer='PCA_img_0.txt',\n",
    "    header=[0],\n",
    "    sep='\\t', skiprows=14*view,nrows=14\n",
    "    )#,skiprows=10,nrows=10'''\n",
    "\n",
    "\n",
    "#list_runs=['fg3_m_12','fg1_m_13']\n",
    "add_on='fg1_m13'#,'fg1_m13']#,'fg1_m13']\n",
    "\n",
    "run=add_on\n",
    "c_0=[]\n",
    "c_1=[]\n",
    "c_2=[]\n",
    "c_3=[]\n",
    "c_4=[]\n",
    "c_5=[]\n",
    "c_6=[]\n",
    "\n",
    "\n",
    "for kz in range(len(df)):\n",
    "    df = pd.io.parsers.read_table(\n",
    "    filepath_or_buffer='LDA_img_ratio_'+str(run)+'_early_late_all_things.txt',#'_view_all.txt',\n",
    "    header=[0],\n",
    "    sep='\\t'\n",
    "    )#,skiprows=10,nrows=10\n",
    "    df.columns = [l for i,l in sorted(feature_dict.items())] + ['Shape Asymmetry']\n",
    "    df.dropna(how=\"all\", inplace=True) # to drop the empty line at file-end\n",
    "\n",
    "    df.drop(df.index[kz])\n",
    "    \n",
    "    for j in range(len(df)):\n",
    "        if df[['Myr']].values[j][0]<40 and df[['Sep']].values[j][0]==0.0 and df[['# Bulges']].values[j][0]==1:#df[['Myr']].values[i][0]\n",
    "\n",
    "        #Then, you can optionally change the class values of all of these viewpoints\n",
    "\n",
    "        #.set_value(index, col, value, \n",
    "            df.set_value(j,'class label',0.0)\n",
    "\n",
    "\n",
    "\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "    \n",
    "    X = df[['Gini','M20','Concentration (C)', 'Asymmetry (A)', 'Clumpiness (S)', 'Sersic N', 'Shape Asymmetry']].values\n",
    "    \n",
    "    from sklearn import preprocessing\n",
    "\n",
    "    \n",
    "\n",
    "    std_scale = preprocessing.StandardScaler().fit(X)\n",
    "    X = std_scale.transform(X)\n",
    "    \n",
    "\n",
    "    n_params=7\n",
    "\n",
    "\n",
    "    y = df['class label'].values\n",
    "    \n",
    "\n",
    "    enc = LabelEncoder()\n",
    "    label_encoder = enc.fit(y)\n",
    "    y = label_encoder.transform(y) + 1\n",
    "\n",
    "\n",
    "    label_dict = {1: 'NonMerger', 2: 'Merger'}\n",
    "    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "    # LDA\n",
    "    sklearn_lda = LDA(priors=[0.94,0.06])\n",
    "    X_lda_sklearn = sklearn_lda.fit_transform(X, y)\n",
    "    dec = sklearn_lda.score(X,y)\n",
    "    prob = sklearn_lda.predict_proba(X)\n",
    "    \n",
    "    coef = sklearn_lda.coef_\n",
    "    \n",
    "    c_0.append(coef[0][0])\n",
    "    c_1.append(coef[0][1])\n",
    "    c_2.append(coef[0][2])\n",
    "    c_3.append(coef[0][3])\n",
    "    c_4.append(coef[0][4])\n",
    "    c_5.append(coef[0][5])\n",
    "    c_6.append(coef[0][6])\n",
    "    \n",
    "'''Now run the full out analysis please'''    \n",
    "df = pd.io.parsers.read_table(\n",
    "    filepath_or_buffer='LDA_img_ratio_'+str(run)+'_early_late_all_things.txt',#'_view_all.txt',\n",
    "    header=[0],\n",
    "    sep='\\t'\n",
    "    )#,skiprows=10,nrows=10\n",
    "df.columns = [l for i,l in sorted(feature_dict.items())] + ['Shape Asymmetry']\n",
    "df.dropna(how=\"all\", inplace=True) # to drop the empty line at file-end\n",
    "\n",
    "for j in range(len(df)):\n",
    "    if df[['Myr']].values[j][0]<40 and df[['Sep']].values[j][0]==0.0 and df[['# Bulges']].values[j][0]==1:#df[['Myr']].values[i][0]\n",
    "\n",
    "\n",
    "        #I use this part to check if there is any separation at these points in time\n",
    "        #Or if there are more than two bulges\n",
    "        #print(df[['class label','Myr','Viewpoint','# Bulges', 'Sep']].values[j])\n",
    "\n",
    "        #Then, you can optionally change the class values of all of these viewpoints\n",
    "\n",
    "        #.set_value(index, col, value, \n",
    "        df.set_value(j,'class label',0.0)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "X = df[['Gini','M20','Concentration (C)', 'Asymmetry (A)', 'Clumpiness (S)', 'Sersic N', 'Shape Asymmetry']].values\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "\n",
    "std_scale = preprocessing.StandardScaler().fit(X)\n",
    "X = std_scale.transform(X)\n",
    "\n",
    "\n",
    "n_params=7\n",
    "\n",
    "\n",
    "y = df['class label'].values\n",
    "\n",
    "\n",
    "enc = LabelEncoder()\n",
    "label_encoder = enc.fit(y)\n",
    "y = label_encoder.transform(y) + 1\n",
    "\n",
    "\n",
    "label_dict = {1: 'NonMerger', 2: 'Merger'}\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "# LDA\n",
    "sklearn_lda = LDA(priors=[0.94,0.06])\n",
    "X_lda_sklearn = sklearn_lda.fit_transform(X, y)\n",
    "dec = sklearn_lda.score(X,y)\n",
    "prob = sklearn_lda.predict_proba(X)\n",
    "\n",
    "coef = sklearn_lda.coef_\n",
    "print(coef)\n",
    "print(np.mean(c_0),np.std(c_0))\n",
    "print(np.mean(c_1),np.std(c_1))\n",
    "print(np.mean(c_2),np.std(c_2))\n",
    "print(np.mean(c_3),np.std(c_3))\n",
    "print(np.mean(c_4),np.std(c_4))\n",
    "print(np.mean(c_5),np.std(c_5))\n",
    "print(np.mean(c_6),np.std(c_6))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run fg3_m12\n",
      "[1. 5. 0. 1. 0.]\n",
      "[ 1. 10.  0.  1.  0.]\n",
      "[ 1. 20.  0.  1.  0.]\n",
      "[ 1. 30.  0.  1.  0.]\n",
      "[1. 5. 1. 1. 0.]\n",
      "[ 1. 10.  1.  1.  0.]\n",
      "[ 1. 20.  1.  1.  0.]\n",
      "[ 1. 30.  1.  1.  0.]\n",
      "[1. 5. 2. 1. 0.]\n",
      "[ 1. 10.  2.  1.  0.]\n",
      "[ 1. 20.  2.  1.  0.]\n",
      "[ 1. 30.  2.  1.  0.]\n",
      "[1. 5. 3. 1. 0.]\n",
      "[ 1. 10.  3.  1.  0.]\n",
      "[ 1. 20.  3.  1.  0.]\n",
      "[1. 5. 4. 1. 0.]\n",
      "[ 1. 10.  4.  1.  0.]\n",
      "[ 1. 20.  4.  1.  0.]\n",
      "[ 1. 30.  4.  1.  0.]\n",
      "[1. 5. 5. 1. 0.]\n",
      "[ 1. 10.  5.  1.  0.]\n",
      "[ 1. 20.  5.  1.  0.]\n",
      "[ 1. 30.  5.  1.  0.]\n",
      "[1. 5. 6. 1. 0.]\n",
      "[ 1. 10.  6.  1.  0.]\n",
      "[ 1. 20.  6.  1.  0.]\n",
      "[0. 5. 0. 1. 0.]\n",
      "[0. 5. 0. 1. 0.]\n",
      "[0. 5. 1. 1. 0.]\n",
      "[0. 5. 1. 1. 0.]\n",
      "[0. 5. 2. 1. 0.]\n",
      "[0. 5. 2. 1. 0.]\n",
      "[0. 5. 3. 1. 0.]\n",
      "[0. 5. 3. 1. 0.]\n",
      "[0. 5. 4. 1. 0.]\n",
      "[0. 5. 4. 1. 0.]\n",
      "[0. 5. 5. 1. 0.]\n",
      "[0. 5. 5. 1. 0.]\n",
      "[0. 5. 6. 1. 0.]\n",
      "[0. 5. 6. 1. 0.]\n",
      "X before norm [[ 0.70978658 -0.79113272  4.42303291 ...  0.34526739  2.9482\n",
      "   0.35841947]\n",
      " [ 0.76440784 -0.84553525  4.3461586  ...  0.34911774  2.7168\n",
      "   0.36333205]\n",
      " [ 0.71709391 -2.65401726  4.51544993 ...  0.27113807  2.7951\n",
      "   0.60966631]\n",
      " ...\n",
      " [ 0.74797309 -1.43463724  2.72034022 ...  0.17243279  1.2946\n",
      "   0.07219759]\n",
      " [ 0.76757161 -1.83924521  2.91788293 ...  0.08947134  0.8205\n",
      "   0.12940721]\n",
      " [ 0.79288243 -2.11533177  3.11624645 ...  0.21211015  1.982\n",
      "   0.09409341]]\n",
      "X after norm [[-0.78457939  2.14009348  0.70981224 ...  0.82466546  1.80290494\n",
      "   0.52860272]\n",
      " [ 0.15125372  2.03500344  0.6196955  ...  0.85103818  1.4863141\n",
      "   0.55442342]\n",
      " [-0.65938194 -1.45846425  0.8181491  ...  0.31692122  1.59344056\n",
      "   1.8491652 ]\n",
      " ...\n",
      " [-0.13032487  0.89702763 -1.28618767 ... -0.35915457 -0.45947458\n",
      "  -0.97578976]\n",
      " [ 0.20545902  0.11544127 -1.05461609 ... -0.92739394 -1.10811641\n",
      "  -0.67509397]\n",
      " [ 0.63911264 -0.41787864 -0.82208231 ... -0.08738689  0.48099451\n",
      "  -0.86070458]]\n",
      "mean accuracy 0.921875\n",
      "[-0.00813543]\n",
      "shape X_lda_sklearn (192, 1)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/beckynevin/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:82: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/Users/beckynevin/anaconda/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:455: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "shape X_qda_sklearn ()\n",
      "0.9947916666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/beckynevin/anaconda/lib/python3.6/site-packages/matplotlib/cbook/deprecation.py:107: MatplotlibDeprecationWarning: Passing one of 'on', 'true', 'off', 'false' as a boolean is deprecated; use an actual boolean (True/False) instead.\n",
      "  warnings.warn(message, mplDeprecation, stacklevel=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 576x396 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1296x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1296x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1296x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1296x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1296x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1296x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1296x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1296x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1296x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1296x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1296x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1296x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1296x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1296x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1296x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1296x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1296x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABC8AAAJsCAYAAADdrYnFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFwAAAECp/R8zLPi85Zz+RwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1296x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.31715482 -2.57240147  4.45759542  6.70784565 -1.12017059  0.26683895\n",
      "   0.69988222]]\n",
      "     class label\n",
      "0              1\n",
      "1              1\n",
      "2              1\n",
      "3              1\n",
      "4              1\n",
      "5              1\n",
      "6              1\n",
      "7              1\n",
      "8              1\n",
      "9              1\n",
      "10             1\n",
      "11             1\n",
      "12             0\n",
      "13             0\n",
      "14             0\n",
      "15             0\n",
      "16             1\n",
      "17             1\n",
      "18             1\n",
      "19             1\n",
      "20             1\n",
      "21             1\n",
      "22             1\n",
      "23             1\n",
      "24             1\n",
      "25             1\n",
      "26             1\n",
      "27             1\n",
      "28             1\n",
      "29             1\n",
      "..           ...\n",
      "162            0\n",
      "163            0\n",
      "164            0\n",
      "165            0\n",
      "166            0\n",
      "167            0\n",
      "168            0\n",
      "169            0\n",
      "170            0\n",
      "171            0\n",
      "172            0\n",
      "173            0\n",
      "174            0\n",
      "175            0\n",
      "176            0\n",
      "177            0\n",
      "178            0\n",
      "179            0\n",
      "180            0\n",
      "181            0\n",
      "182            0\n",
      "183            0\n",
      "184            0\n",
      "185            0\n",
      "186            0\n",
      "187            0\n",
      "188            0\n",
      "189            0\n",
      "190            0\n",
      "191            0\n",
      "\n",
      "[192 rows x 1 columns]\n",
      "run fg1_m13\n",
      "[ 1. 10.  0.  1.  0.]\n",
      "[ 1. 10.  1.  1.  0.]\n",
      "[ 1. 10.  2.  1.  0.]\n",
      "[ 1. 10.  3.  1.  0.]\n",
      "[ 1. 10.  4.  1.  0.]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/beckynevin/anaconda/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/beckynevin/anaconda/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/Users/beckynevin/anaconda/lib/python3.6/site-packages/numpy/core/_methods.py:135: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  keepdims=keepdims)\n",
      "/Users/beckynevin/anaconda/lib/python3.6/site-packages/numpy/core/_methods.py:105: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/Users/beckynevin/anaconda/lib/python3.6/site-packages/numpy/core/_methods.py:127: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ 1. 10.  5.  1.  0.]\n",
      "[ 1. 10.  6.  1.  0.]\n",
      "[0. 5. 0. 1. 0.]\n",
      "[0. 5. 0. 1. 0.]\n",
      "[0. 5. 1. 1. 0.]\n",
      "[0. 5. 1. 1. 0.]\n",
      "[0. 5. 2. 1. 0.]\n",
      "[0. 5. 2. 1. 0.]\n",
      "[0. 5. 3. 1. 0.]\n",
      "[0. 5. 3. 1. 0.]\n",
      "[0. 5. 4. 1. 0.]\n",
      "[0. 5. 4. 1. 0.]\n",
      "[0. 5. 5. 1. 0.]\n",
      "[0. 5. 5. 1. 0.]\n",
      "[0. 5. 6. 1. 0.]\n",
      "[0. 5. 6. 1. 0.]\n",
      "X before norm [[ 0.61020128 -1.84076285  2.72034022 ...  0.03551586  0.8552\n",
      "   0.07418257]\n",
      " [ 0.61313577 -1.86532189  2.79654005 ...  0.12571824  0.8864\n",
      "   0.16252166]\n",
      " [ 0.72319029 -1.57674979  2.82135715 ...  0.23993998  0.5103\n",
      "   0.47637   ]\n",
      " ...\n",
      " [ 0.69359342 -1.83051587  2.72034022 ...  0.0593231   0.8355\n",
      "   0.04984751]\n",
      " [ 0.71240611 -1.788465    2.72034022 ...  0.08271237  0.9579\n",
      "   0.04851613]\n",
      " [ 0.72429504 -1.88212437  3.01029996 ...  0.30658355  1.6703\n",
      "   0.08324348]]\n",
      "X after norm [[-2.03854239  0.12943664 -1.51599442 ... -1.48483818 -1.21993582\n",
      "  -0.9141491 ]\n",
      " [-1.99062564  0.08372735 -1.41401758 ... -0.7766804  -1.16793862\n",
      "  -0.53664549]\n",
      " [-0.19356923  0.6208176  -1.38080532 ...  0.12004773 -1.7947381\n",
      "   0.80453742]\n",
      " ...\n",
      " [-0.67685004  0.14850831 -1.51599442 ... -1.29793311 -1.25276738\n",
      "  -1.01814125]\n",
      " [-0.36966167  0.22677336 -1.51599442 ... -1.11430947 -1.0487784\n",
      "  -1.02383071]\n",
      " [-0.17552991  0.0524546  -1.12794663 ...  0.64325073  0.1384908\n",
      "  -0.8754287 ]]\n",
      "mean accuracy 0.7010309278350515\n",
      "[-1.04187521]\n",
      "shape X_lda_sklearn (194, 1)\n",
      "shape X_qda_sklearn ()\n",
      "0.9432989690721649\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 576x396 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABC8AAAJfCAYAAABfQ/zGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 1296x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.02264439 -2.06155472  2.21632597  2.5695055   0.44245812  0.36149426\n",
      "   1.24281559]]\n",
      "     class label\n",
      "0              0\n",
      "1              1\n",
      "2              1\n",
      "3              1\n",
      "4              1\n",
      "5              1\n",
      "6              1\n",
      "7              1\n",
      "8              1\n",
      "9              1\n",
      "10             1\n",
      "11             1\n",
      "12             1\n",
      "13             1\n",
      "14             1\n",
      "15             1\n",
      "16             1\n",
      "17             1\n",
      "18             1\n",
      "19             1\n",
      "20             1\n",
      "21             1\n",
      "22             0\n",
      "23             1\n",
      "24             1\n",
      "25             1\n",
      "26             1\n",
      "27             1\n",
      "28             1\n",
      "29             1\n",
      "..           ...\n",
      "164            0\n",
      "165            0\n",
      "166            0\n",
      "167            0\n",
      "168            0\n",
      "169            0\n",
      "170            0\n",
      "171            0\n",
      "172            0\n",
      "173            0\n",
      "174            0\n",
      "175            0\n",
      "176            0\n",
      "177            0\n",
      "178            0\n",
      "179            0\n",
      "180            0\n",
      "181            0\n",
      "182            0\n",
      "183            0\n",
      "184            0\n",
      "185            0\n",
      "186            0\n",
      "187            0\n",
      "188            0\n",
      "189            0\n",
      "190            0\n",
      "191            0\n",
      "192            0\n",
      "193            0\n",
      "\n",
      "[194 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "A tool for separating things in the classification out by viewing angle or other things like\n",
    "myr snapshot :)\n",
    "'''\n",
    "\n",
    "'''\n",
    "~~~\n",
    "Now just for the imaging part of it!\n",
    "~~~\n",
    "'''\n",
    "import numpy.ma as ma\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_mean_and_CI(mean, lb, ub, color_mean=None, color_shading=None):\n",
    "        # plot the shaded range of the confidence intervals\n",
    "        plt.fill_between(range(mean.shape[0]), ub, lb,\n",
    "                         color=color_shading, alpha=.5)\n",
    "        # plot the mean on top\n",
    "        plt.plot(mean, color_mean)\n",
    "\n",
    "\n",
    "feature_dict = {i:label for i,label in zip(\n",
    "                range(14),\n",
    "                  ('Counter',\n",
    "                  'Image',\n",
    "                  'class label',\n",
    "                  'Myr',\n",
    "                  'Viewpoint',\n",
    "                '# Bulges',\n",
    "                   'Sep',\n",
    "                   'Flux Ratio',\n",
    "                  'Gini',\n",
    "                  'M20',\n",
    "                  'Concentration (C)',\n",
    "                  'Asymmetry (A)',\n",
    "                  'Clumpiness (S)',\n",
    "                  'Sersic N',\n",
    "                  'Shape Asymmetry (A_S)'))}\n",
    "\n",
    "#Counter\tImage\tMerger (0 = no, 1 = yes)\tMyr\tViewpoint\tGini\tM20\tC\tA\tS\tSersic n\n",
    "'''view=0\n",
    "df = pd.io.parsers.read_table(\n",
    "    filepath_or_buffer='PCA_img_0.txt',\n",
    "    header=[0],\n",
    "    sep='\\t', skiprows=14*view,nrows=14\n",
    "    )#,skiprows=10,nrows=10'''\n",
    "\n",
    "\n",
    "#list_runs=['fg3_m_12','fg1_m_13']\n",
    "list_runs=['fg3_m12','fg1_m13']#,'fg1_m13']\n",
    "\n",
    "for i in range(len(list_runs)):\n",
    "   \n",
    "    add_on=list_runs[i]\n",
    "    print('run', add_on)\n",
    "\n",
    "\n",
    "    run=list_runs[i]\n",
    "    df = pd.io.parsers.read_table(\n",
    "        filepath_or_buffer='LDA_img_ratio_'+str(run)+'_early_late_all_things.txt',#'_view_all.txt',\n",
    "        header=[0],\n",
    "        sep='\\t'\n",
    "        )#,skiprows=10,nrows=10\n",
    "    df.columns = [l for i,l in sorted(feature_dict.items())] + ['Shape Asymmetry']\n",
    "    df.dropna(how=\"all\", inplace=True) # to drop the empty line at file-end\n",
    "    \n",
    "    for j in range(len(df)):\n",
    "        if df[['Myr']].values[j][0]<40 and df[['Sep']].values[j][0]==0.0 and df[['# Bulges']].values[j][0]==1:#df[['Myr']].values[i][0]\n",
    "            \n",
    "            \n",
    "            #I use this part to check if there is any separation at these points in time\n",
    "            #Or if there are more than two bulges\n",
    "            print(df[['class label','Myr','Viewpoint','# Bulges', 'Sep']].values[j])\n",
    "            \n",
    "            #Then, you can optionally change the class values of all of these viewpoints\n",
    "            \n",
    "            #.set_value(index, col, value, \n",
    "            df.set_value(j,'class label',0.0)\n",
    "    \n",
    "    \n",
    "\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "    X = df[['Gini','M20','Concentration (C)', 'Asymmetry (A)', 'Clumpiness (S)', 'Sersic N', 'Shape Asymmetry']].values\n",
    "    \n",
    "    from sklearn import preprocessing\n",
    "\n",
    "    print('X before norm', X)\n",
    "\n",
    "    std_scale = preprocessing.StandardScaler().fit(X)\n",
    "    X = std_scale.transform(X)\n",
    "    print('X after norm', X)\n",
    "\n",
    "    n_params=7\n",
    "\n",
    "\n",
    "    y = df['class label'].values\n",
    "    \n",
    "\n",
    "    enc = LabelEncoder()\n",
    "    label_encoder = enc.fit(y)\n",
    "    y = label_encoder.transform(y) + 1\n",
    "\n",
    "\n",
    "    label_dict = {1: 'NonMerger', 2: 'Merger'}\n",
    "    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "    # LDA\n",
    "    sklearn_lda = LDA(priors=[0.94,0.04])\n",
    "    X_lda_sklearn = sklearn_lda.fit_transform(X, y)\n",
    "    dec = sklearn_lda.score(X,y)\n",
    "    prob = sklearn_lda.predict_proba(X)\n",
    "    \n",
    "    coef = sklearn_lda.coef_\n",
    "    inter = sklearn_lda.intercept_\n",
    "    class_label = sklearn_lda.classes_\n",
    "    \n",
    "    \n",
    "   \n",
    "    print('mean accuracy',dec)#mean accuracy on the given test data and labels.\n",
    "    print('~~~Coefficients and Intercepts~~~')\n",
    "    print(coef,inter)\n",
    "    \n",
    "    from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "    \n",
    "    # QDA\n",
    "    sklearn_qda = QDA(priors=[0.9,0.1])\n",
    "    X_qda_sklearn = sklearn_qda.fit(X, y)\n",
    "    print('shape X_lda_sklearn', np.shape(X_lda_sklearn))\n",
    "    print('shape X_qda_sklearn', np.shape(X_qda_sklearn))\n",
    "    dec_qda = sklearn_qda.score(X,y)\n",
    "    \n",
    "    #coef = sklearn_qda.coef_\n",
    "    #inter = sklearn_qda.intercept_\n",
    "    print(dec_qda)#mean accuracy on the given test data and labels.\n",
    "\n",
    "    '''Make a histogram'''\n",
    "    from scipy import stats\n",
    "    import seaborn as sns\n",
    "    plt.clf()\n",
    "    fig = plt.figure(figsize=(18,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    X_lda_1=[]\n",
    "    X_lda_2=[]\n",
    "    for j in range(len(X_lda_sklearn)):\n",
    "        if y[j] ==1:\n",
    "            X_lda_1.append(X_lda_sklearn[j][0])\n",
    "        else:\n",
    "            X_lda_2.append(X_lda_sklearn[j][0])\n",
    "    input_hist=X_lda_sklearn\n",
    "    \n",
    "    ax.hist(X_lda_1, label='NonMerger',  color=sns.xkcd_rgb[\"sky blue\"],alpha = 0.75)\n",
    "    ax.hist(X_lda_2, label='Merger',  color=sns.xkcd_rgb[\"salmon\"],alpha = 0.75)\n",
    "\n",
    "    '''for label,col in zip(range(1,4),  ('blue', 'red')):\n",
    "        input_hist=X_lda_sklearn\n",
    "        input_all=X_lda_sklearn\n",
    "        ax.hist(input_hist,\n",
    "                       color=col,\n",
    "                       label='class %s' %label_dict[label],\n",
    "                       alpha=0.5,)#bins=bins,\n",
    "        xt = plt.xticks()[0]  \n",
    "        xmin, xmax = -0.1,0.7#min(xt), max(xt)  \n",
    "        lnspc = np.linspace(xmin, xmax, len(input_hist))\n",
    "\n",
    "        # lets try the normal distribution first\n",
    "        m, s = stats.norm.fit(input_hist) # get mean and standard deviation  \n",
    "        pdf_g = stats.norm.pdf(lnspc, m, s) # now get theoretical values in our interval  \n",
    "        #ax.plot(lnspc, pdf_g,  color=col) # plot it\n",
    "\n",
    "\n",
    "\n",
    "    ylims = ax.get_ylim()\n",
    "\n",
    "    # plot annotation\n",
    "    leg = ax.legend(loc='upper right', fancybox=True, fontsize=8)\n",
    "    leg.get_frame().set_alpha(0.5)\n",
    "    ax.set_ylim([0, max(ylims)+2])'''\n",
    "\n",
    "    ax.set_xlabel('LD1', size=20)\n",
    "    #ax.set_title('Histogram #%s' %str(cnt+1), size=20)\n",
    "\n",
    "    # hide axis ticks\n",
    "    ax.tick_params(axis=\"both\", which=\"both\", bottom=\"off\", top=\"off\",  \n",
    "            labelbottom=\"on\", left=\"off\", right=\"off\", labelleft=\"on\", labelsize=20)\n",
    "\n",
    "    # remove axis spines\n",
    "    ax.spines[\"top\"].set_visible(False)  \n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines[\"bottom\"].set_visible(False)\n",
    "    ax.spines[\"left\"].set_visible(False)    \n",
    "\n",
    "    ax.set_ylabel('Count', size=20)\n",
    "    \n",
    "    \n",
    "    plt.legend(loc=\"upper right\", fontsize=20)\n",
    "    #fig.tight_layout() \n",
    "    #plt.annotate(str(add_on), xy=(0.02,0.95),xycoords='axes fraction', size=20)\n",
    "    #plt.annotate('Mean Accuracy = '+str(dec), xy=(0.02,0.9),xycoords='axes fraction', size=20)\n",
    "    #frame1 = plt.gca()\n",
    "    if run=='fg1_m_13':\n",
    "        plt.title('FG1M13')\n",
    "    if run=='fg3_m12':\n",
    "        plt.title('FG3M12')\n",
    "    plt.show()\n",
    "    print(coef)\n",
    "    #plt.savefig('../MaNGA_Papers/Paper_I/Marginalized_img_'+str(run)+'.pdf')\n",
    "    #plt.clf()\n",
    "    \n",
    "    '''Also, making those mountain plots for the imaging runs'''\n",
    "    \n",
    "    \n",
    "    \n",
    "    '''Now measure LD1 for every row and then plot that'''\n",
    "    import seaborn as sns\n",
    "    \n",
    "\n",
    "    n_params=7\n",
    "\n",
    "\n",
    "\n",
    "    #coef is how you get the eigvecs (doesn't matter what slope offset is)\n",
    "    #print('real eigvecs',(eigvec_sc.real))\n",
    "    #print(len(X_lda[:,0].real[y==2]))#[y == label]\n",
    "    xs=[]\n",
    "    LDA1=[]\n",
    "    if run=='fg3_m_12':\n",
    "        myr=[170,180,185,190,195,205,210,220,225,230,240,250,260]\n",
    "        myr_non=[5,200]\n",
    "    if run=='fg3_m12':\n",
    "        myr=[5,10,20,30,40,60,80,100,120,140,160,170,180,185,190,195,205,210,220,225,230,240,250,260]\n",
    "        myr_non=[5,100,200]\n",
    "        myr_non=[5,10,20,30,100,200]\n",
    "    if run=='fg1_m13':\n",
    "        myr=[10,40,50,60,70,90,100,120,130,140,170,180,185,190,195,200,205,210,215,220,225,230,235,240,250,260,270,280,290,300,310,320,330,340,350]\n",
    "        myr_non=[5,10,100,200]\n",
    "    if run=='fg1_m_13':\n",
    "        myr=[40,195,210,215,220,225,230,235,240,250,260,270,280,290,300,310,320,330,340,350]\n",
    "        myr_non=[5,200]\n",
    "    my_lists = {key:[] for key in myr}\n",
    "    my_lists_none = {key:[] for key in myr_non}\n",
    "    my_lists_non = []\n",
    "    separations = {key:[] for key in myr}\n",
    "\n",
    "    print(df[['class label']])\n",
    "    #STOP\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        if df[['class label']].values[i]==0:\n",
    "            my_lists_non.append(X_lda_sklearn[i][0])\n",
    "            my_lists_none[df[['Myr']].values[i][0]].append(X_lda_sklearn[i][0])\n",
    "            continue\n",
    "        my_lists[df[['Myr']].values[i][0]].append(X_lda_sklearn[i][0])\n",
    "        separations[df[['Myr']].values[i][0]].append(df[['Sep']].values[i][0])\n",
    "        L=X_lda_sklearn[i][0]\n",
    "        #df[['Gini']].values[i][0]*coef[0][0]+df[['M20']].values[i][0]*coef[0][1]+df[['Concentration (C)']].values[i][0]*coef[0][2]+df[['Asymmetry (A)']].values[i][0]*coef[0][3]+df[['Clumpiness (S)']].values[i][0]*coef[0][4]+df[['Sersic N']].values[i][0]*coef[0][5]+df[['Shape Asymmetry']].values[i][0]*coef[0][6]\n",
    "        LDA1.append(L)\n",
    "        xs.append(df[['Myr']].values[i][0])\n",
    "\n",
    "    #print(mean(my_lists[180]))\n",
    "    \n",
    "    '''Make the beautiful list of colors'''\n",
    "    # These are the \"Tableau 20\" colors as RGB.    \n",
    "    tableau20 = [(31, 119, 180), (174, 199, 232), (255, 127, 14), (255, 187, 120),    \n",
    "                 (44, 160, 44), (152, 223, 138), (214, 39, 40), (255, 152, 150),    \n",
    "                 (148, 103, 189), (197, 176, 213), (140, 86, 75), (196, 156, 148),    \n",
    "                 (227, 119, 194), (247, 182, 210), (127, 127, 127), (199, 199, 199),    \n",
    "                 (188, 189, 34), (219, 219, 141), (23, 190, 207), (158, 218, 229)]    \n",
    "\n",
    "    # Scale the RGB values to the [0, 1] range, which is the format matplotlib accepts.    \n",
    "    for i in range(len(tableau20)):    \n",
    "        r, g, b = tableau20[i]    \n",
    "        tableau20[i] = (r / 255., g / 255., b / 255.)    \n",
    "    \n",
    "    mean_non=np.mean(my_lists_non)+np.std(my_lists_non)\n",
    "    means=[]\n",
    "    std=[]\n",
    "    separation_value=[]\n",
    "    plt.clf()\n",
    "    \n",
    "    for i in range(len(myr)):\n",
    "        means.append(np.mean(my_lists[myr[i]]))\n",
    "        std.append(np.std(my_lists[myr[i]]))\n",
    "        separation_value.append(np.mean(separations[myr[i]]))\n",
    "    for i in range(len(myr_non)):\n",
    "        myr_plot=np.linspace(myr_non[i]/100,myr_non[i]/100,len(my_lists_none[myr_non[i]]))\n",
    "        #np.full((3, 5), 7) that last value is your fill\n",
    "        plt.scatter(myr_plot,my_lists_none[myr_non[i]], color=tableau20[i])\n",
    "    \n",
    "    \n",
    "    means=np.array(means)\n",
    "    std=np.array(std)\n",
    "    myr=np.array(myr)\n",
    "    \n",
    "    if run=='fg1_m13':\n",
    "        '''Try to replot with a dimensionless y axis with just means and std'''\n",
    "        rescale_y_mean=0#-1.7583e-01\n",
    "        rescale_y_pm=7.13475416061/2\n",
    "\n",
    "\n",
    "        new_means=np.array([(x-rescale_y_mean) for x in means])\n",
    "        #new_means=ma.masked_where(math.isnan(new_means),new_means)\n",
    "        #print('these are apparently the myrs', myr/100)\n",
    "        #print('these are the fills', new_means)\n",
    "        \n",
    "        plt.plot(myr/100, new_means, color=sns.xkcd_rgb[\"amber\"])\n",
    "        plt.fill_between(myr/100, (new_means-std), (new_means+std),alpha=.5, color=sns.xkcd_rgb[\"amber\"])\n",
    "        plt.xlabel(r'Merger Timeline [Gyr]', size=15)\n",
    "        plt.ylabel(r'Detection Sensitivity (LD1)', size=15)\n",
    "        #plt.axvline(x=220/100, color='black', ls='--')\n",
    "        plt.axvline(x=215/100, color='black', ls='--')\n",
    "        plt.axvline(x=280/100, color='black', ls='--')\n",
    "\n",
    "        '''width and height 3.17421463155 0.87713041243\n",
    "        pos [ -1.6779e+00   2.6963e-16]\n",
    "        width and height 5.56843021209 2.94988106646\n",
    "        pos [  2.7636e-01   1.2800e-16]'''\n",
    "\n",
    "\n",
    "        '''fg1_m_13:\n",
    "        width and height 1.98151143695 1.33924739273\n",
    "        pos [  1.4067e+00   6.6911e-16]\n",
    "        width and height 7.13475416061 2.66110273731\n",
    "        pos [ -1.7583e-01  -1.3878e-16]'''\n",
    "\n",
    "        plt.axhline(y=mean_non, color='black')\n",
    "        #plt.annotate(r'$\\mu_{\\mathrm{Merger}}$', xy=(0.02,0.53), xycoords='axes fraction', size=15)\n",
    "        #plt.title(str(run))\n",
    "        plt.annotate('FG1M13', xy=(0.02,1.02), xycoords='axes fraction', size=20)\n",
    "        plt.annotate('Early', xy=(0.25,0.95), xycoords='axes fraction', size=9)\n",
    "        plt.annotate('Late', xy=(0.63,0.95), xycoords='axes fraction',size=9)\n",
    "        plt.annotate('Post Coalescence', xy=(0.76,1.02), xycoords='axes fraction', size=9)\n",
    "\n",
    "    if run=='fg1_m_13':\n",
    "        '''Try to replot with a dimensionless y axis with just means and std'''\n",
    "        rescale_y_mean=0#-1.7583e-01\n",
    "        rescale_y_pm=7.13475416061/2\n",
    "\n",
    "\n",
    "        new_means=np.array([(x-rescale_y_mean) for x in means])\n",
    "\n",
    "        \n",
    "\n",
    "        plt.plot(myr/100, new_means, color=sns.xkcd_rgb[\"amber\"])\n",
    "        plt.fill_between(myr/100, (new_means-std), (new_means+std),alpha=.5, color=sns.xkcd_rgb[\"amber\"])\n",
    "        plt.xlabel(r'Merger Timeline [Gyr]', size=15)\n",
    "        plt.ylabel(r'Detection Sensitivity (LD1)', size=15)\n",
    "        #plt.axvline(x=220/100, color='black', ls='--')\n",
    "        plt.axvline(x=215/100, color='black', ls='--')\n",
    "        plt.axvline(x=280/100, color='black', ls='--')\n",
    "\n",
    "        '''width and height 3.17421463155 0.87713041243\n",
    "        pos [ -1.6779e+00   2.6963e-16]\n",
    "        width and height 5.56843021209 2.94988106646\n",
    "        pos [  2.7636e-01   1.2800e-16]'''\n",
    "\n",
    "\n",
    "        '''fg1_m_13:\n",
    "        width and height 1.98151143695 1.33924739273\n",
    "        pos [  1.4067e+00   6.6911e-16]\n",
    "        width and height 7.13475416061 2.66110273731\n",
    "        pos [ -1.7583e-01  -1.3878e-16]'''\n",
    "\n",
    "        ys_LD1=np.array([-1.7583e-01 for x in myr])\n",
    "        #plt.plot(myr/100,ys_LD1)\n",
    "        #plt.fill_between(myr/100, ys_LD1-0.588/2, ys_LD1+0.588/2,alpha=.5)\n",
    "        plt.axhline(y=mean_non, color='black')\n",
    "        #plt.annotate(r'$\\mu_{\\mathrm{Merger}}$', xy=(0.02,0.53), xycoords='axes fraction', size=15)\n",
    "        #plt.title(str(run))\n",
    "        plt.annotate('FG1M13', xy=(0.02,1.02), xycoords='axes fraction', size=20)\n",
    "        plt.annotate('Early', xy=(0.25,0.95), xycoords='axes fraction', size=9)\n",
    "        plt.annotate('Late', xy=(0.63,0.95), xycoords='axes fraction',size=9)\n",
    "        plt.annotate('Post Coalescence', xy=(0.76,1.02), xycoords='axes fraction', size=9)\n",
    "    if run=='fg3_m_12':\n",
    "        '''Try to replot with a dimensionless y axis with just means and std'''\n",
    "        rescale_y_mean=0#2.7636e-01\n",
    "        rescale_y_pm=5.5684302120/2\n",
    "\n",
    "\n",
    "        new_means=np.array([(x-rescale_y_mean) for x in means])\n",
    "\n",
    "        \n",
    "\n",
    "        plt.plot(myr/100, new_means, color='red')\n",
    "        plt.fill_between(myr/100, (new_means-std), (new_means+std),alpha=.5, color='red')\n",
    "        plt.xlabel(r'Merger Timeline [Gyr]', size=15)\n",
    "        plt.ylabel(r'Detection Sensitivity (LD1)', size=15)\n",
    "        plt.axvline(x=220/100, color='black', ls='--')\n",
    "        plt.axvline(x=180/100, color='black', ls='--')\n",
    "\n",
    "\n",
    "\n",
    "        '''width and height 3.17421463155 0.87713041243\n",
    "        pos [ -1.6779e+00   2.6963e-16]\n",
    "        width and height 5.56843021209 2.94988106646\n",
    "        pos [  2.7636e-01   1.2800e-16]'''\n",
    "\n",
    "\n",
    "        #plt.axhline(y=0, color='black')\n",
    "\n",
    "        #plt.annotate(r'$\\mu_{\\mathrm{Merger}}$', xy=(0.15,0.53), xycoords='axes fraction', size=15)\n",
    "        #plt.title(str(run))\n",
    "        plt.axhline(y=mean_non, color='black')\n",
    "        plt.annotate('FG3M12', xy=(0.02,1.02), xycoords='axes fraction', size=20)\n",
    "        plt.annotate('Early', xy=(0.03,0.95), xycoords='axes fraction', size=9)\n",
    "        plt.annotate('Late', xy=(0.3,0.95), xycoords='axes fraction',size=9)\n",
    "        plt.annotate('Post Coalescence', xy=(0.65,0.95), xycoords='axes fraction', size=9)\n",
    "    if run=='fg3_m12':\n",
    "    \n",
    "        '''Try to replot with a dimensionless y axis with just means and std'''\n",
    "        rescale_y_mean=0#2.7636e-01\n",
    "        rescale_y_pm=5.5684302120/2\n",
    "\n",
    "\n",
    "        new_means=np.array([(x-rescale_y_mean) for x in means])\n",
    "\n",
    "        \n",
    "\n",
    "        plt.plot(myr/100, new_means, color='red')\n",
    "        plt.fill_between(myr/100, (new_means-std), (new_means+std),alpha=.5, color='red')\n",
    "        plt.xlabel(r'Merger Timeline [Gyr]', size=15)\n",
    "        plt.ylabel(r'Detection Sensitivity (LD1)', size=15)\n",
    "        plt.axvline(x=220/100, color='black', ls='--')\n",
    "        plt.axvline(x=180/100, color='black', ls='--')\n",
    "\n",
    "\n",
    "\n",
    "        '''width and height 3.17421463155 0.87713041243\n",
    "        pos [ -1.6779e+00   2.6963e-16]\n",
    "        width and height 5.56843021209 2.94988106646\n",
    "        pos [  2.7636e-01   1.2800e-16]'''\n",
    "\n",
    "\n",
    "        #plt.axhline(y=0, color='black')\n",
    "\n",
    "        #plt.annotate(r'$\\mu_{\\mathrm{Merger}}$', xy=(0.15,0.53), xycoords='axes fraction', size=15)\n",
    "        #plt.title(str(run))\n",
    "        plt.axhline(y=mean_non, color='black')\n",
    "        plt.annotate('FG3M12', xy=(0.02,1.02), xycoords='axes fraction', size=20)\n",
    "        plt.annotate('Early', xy=(0.63,0.97), xycoords='axes fraction', size=9)\n",
    "        plt.annotate('Late', xy=(0.7,0.97), xycoords='axes fraction',size=9)\n",
    "        plt.annotate('Post Coalescence', xy=(0.85,1.01), xycoords='axes fraction', size=9)\n",
    "    #plt.ylim([-1,1])\n",
    "    #plt.xlim([min(myr)/100,max(myr)/100])\n",
    "    #plt.xlim([0,])\n",
    "    frame1 = plt.gca()\n",
    "    #frame1.axes.xaxis.set_ticklabels([])\n",
    "    frame1.axes.yaxis.set_ticklabels([])\n",
    "    #frame1.axes.yaxis.set_ticks([])\n",
    "    plt.savefig('../MaNGA_Papers/Paper_I/Mountain_plot_imaging_priors_colors_'+str(run)+'.pdf')\n",
    "\n",
    "\n",
    "\n",
    "    '''Try to replot with a dimensionless y axis with just means and std'''\n",
    "    \n",
    "    \n",
    "#    savefig('../MaNGA_Papers/Paper_I/Bayesian_Hist_'+str(run)+'.pdf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run fg3_m15\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'LDA_img_ratio_fg3_m15_early_late_all_things.txt' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-551fe2f0aa18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'LDA_img_ratio_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_early_late_all_things.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m#'_view_all.txt',\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         )#,skiprows=10,nrows=10\n\u001b[1;32m     71\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Shape Asymmetry'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'LDA_img_ratio_fg3_m15_early_late_all_things.txt' does not exist"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Now trying to do the same thing for different viewing angles\n",
    "'''\n",
    "'''\n",
    "A tool for separating things in the classification out by viewing angle or other things like\n",
    "myr snapshot :)\n",
    "'''\n",
    "\n",
    "'''\n",
    "~~~\n",
    "Now just for the imaging part of it!\n",
    "~~~\n",
    "'''\n",
    "import numpy.ma as ma\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_mean_and_CI(mean, lb, ub, color_mean=None, color_shading=None):\n",
    "        # plot the shaded range of the confidence intervals\n",
    "        plt.fill_between(range(mean.shape[0]), ub, lb,\n",
    "                         color=color_shading, alpha=.5)\n",
    "        # plot the mean on top\n",
    "        plt.plot(mean, color_mean)\n",
    "\n",
    "\n",
    "feature_dict = {i:label for i,label in zip(\n",
    "                range(14),\n",
    "                  ('Counter',\n",
    "                  'Image',\n",
    "                  'class label',\n",
    "                  'Myr',\n",
    "                  'Viewpoint',\n",
    "                '# Bulges',\n",
    "                   'Sep',\n",
    "                   'Flux Ratio',\n",
    "                  'Gini',\n",
    "                  'M20',\n",
    "                  'Concentration (C)',\n",
    "                  'Asymmetry (A)',\n",
    "                  'Clumpiness (S)',\n",
    "                  'Sersic N',\n",
    "                  'Shape Asymmetry (A_S)'))}\n",
    "\n",
    "#Counter\tImage\tMerger (0 = no, 1 = yes)\tMyr\tViewpoint\tGini\tM20\tC\tA\tS\tSersic n\n",
    "'''view=0\n",
    "df = pd.io.parsers.read_table(\n",
    "    filepath_or_buffer='PCA_img_0.txt',\n",
    "    header=[0],\n",
    "    sep='\\t', skiprows=14*view,nrows=14\n",
    "    )#,skiprows=10,nrows=10'''\n",
    "\n",
    "\n",
    "#list_runs=['fg3_m_12','fg1_m_13']\n",
    "list_runs=['fg3_m15','fg3_m12','fg1_m13']#,'fg1_m13']\n",
    "\n",
    "for i in range(len(list_runs)):\n",
    "   \n",
    "    add_on=list_runs[i]\n",
    "    print('run', add_on)\n",
    "\n",
    "\n",
    "    run=list_runs[i]\n",
    "    df = pd.io.parsers.read_table(\n",
    "        filepath_or_buffer='LDA_img_ratio_'+str(run)+'_early_late_all_things.txt',#'_view_all.txt',\n",
    "        header=[0],\n",
    "        sep='\\t'\n",
    "        )#,skiprows=10,nrows=10\n",
    "    df.columns = [l for i,l in sorted(feature_dict.items())] + ['Shape Asymmetry']\n",
    "    df.dropna(how=\"all\", inplace=True) # to drop the empty line at file-end\n",
    "    \n",
    "    for j in range(len(df)):\n",
    "        if df[['Myr']].values[j][0]<40 and df[['Sep']].values[j][0]==0.0 and df[['# Bulges']].values[j][0]==1:#df[['Myr']].values[i][0]\n",
    "            \n",
    "            \n",
    "            #I use this part to check if there is any separation at these points in time\n",
    "            #Or if there are more than two bulges\n",
    "            #print(df[['class label','Myr','Viewpoint','# Bulges', 'Sep']].values[j])\n",
    "            \n",
    "            #Then, you can optionally change the class values of all of these viewpoints\n",
    "            \n",
    "            #.set_value(index, col, value, \n",
    "            df.set_value(j,'class label',0.0)\n",
    "    \n",
    "    \n",
    "\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "    X = df[['Gini','M20','Concentration (C)', 'Asymmetry (A)', 'Clumpiness (S)', 'Sersic N', 'Shape Asymmetry']].values\n",
    "    \n",
    "    from sklearn import preprocessing\n",
    "\n",
    "    \n",
    "\n",
    "    std_scale = preprocessing.StandardScaler().fit(X)\n",
    "    X = std_scale.transform(X)\n",
    "    \n",
    "    n_params=7\n",
    "\n",
    "\n",
    "    y = df['class label'].values\n",
    "    \n",
    "\n",
    "    enc = LabelEncoder()\n",
    "    label_encoder = enc.fit(y)\n",
    "    y = label_encoder.transform(y) + 1\n",
    "\n",
    "\n",
    "    label_dict = {1: 'NonMerger', 2: 'Merger'}\n",
    "    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "    # LDA\n",
    "    sklearn_lda = LDA(priors=[0.94,0.04])\n",
    "    X_lda_sklearn = sklearn_lda.fit_transform(X, y)\n",
    "    dec = sklearn_lda.score(X,y)\n",
    "    prob = sklearn_lda.predict_proba(X)\n",
    "    \n",
    "    coef = sklearn_lda.coef_\n",
    "    inter = sklearn_lda.intercept_\n",
    "    class_label = sklearn_lda.classes_\n",
    "    \n",
    "    \n",
    "   \n",
    "    print('mean accuracy',dec)#mean accuracy on the given test data and labels.\n",
    "    \n",
    "    print(inter)\n",
    "    \n",
    "    from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "    \n",
    "    # QDA\n",
    "    sklearn_qda = QDA(priors=[0.94,0.04])\n",
    "    X_qda_sklearn = sklearn_qda.fit(X, y)\n",
    "    dec_qda = sklearn_qda.score(X,y)\n",
    "    \n",
    "    #coef = sklearn_qda.coef_\n",
    "    #inter = sklearn_qda.intercept_\n",
    "    print(dec_qda)#mean accuracy on the given test data and labels.\n",
    "\n",
    "    '''Make a histogram'''\n",
    "    from scipy import stats\n",
    "    import seaborn as sns\n",
    "    plt.clf()\n",
    "    fig = plt.figure(figsize=(18,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    X_lda_1=[]\n",
    "    X_lda_2=[]\n",
    "    for j in range(len(X_lda_sklearn)):\n",
    "        if y[j] ==1:\n",
    "            X_lda_1.append(X_lda_sklearn[j][0])\n",
    "        else:\n",
    "            X_lda_2.append(X_lda_sklearn[j][0])\n",
    "    input_hist=X_lda_sklearn\n",
    "    \n",
    "    ax.hist(X_lda_1, label='NonMerger',  color=sns.xkcd_rgb[\"sky blue\"],alpha = 0.75)\n",
    "    ax.hist(X_lda_2, label='Merger',  color=sns.xkcd_rgb[\"salmon\"],alpha = 0.75)\n",
    "\n",
    "    '''for label,col in zip(range(1,4),  ('blue', 'red')):\n",
    "        input_hist=X_lda_sklearn\n",
    "        input_all=X_lda_sklearn\n",
    "        ax.hist(input_hist,\n",
    "                       color=col,\n",
    "                       label='class %s' %label_dict[label],\n",
    "                       alpha=0.5,)#bins=bins,\n",
    "        xt = plt.xticks()[0]  \n",
    "        xmin, xmax = -0.1,0.7#min(xt), max(xt)  \n",
    "        lnspc = np.linspace(xmin, xmax, len(input_hist))\n",
    "\n",
    "        # lets try the normal distribution first\n",
    "        m, s = stats.norm.fit(input_hist) # get mean and standard deviation  \n",
    "        pdf_g = stats.norm.pdf(lnspc, m, s) # now get theoretical values in our interval  \n",
    "        #ax.plot(lnspc, pdf_g,  color=col) # plot it\n",
    "\n",
    "\n",
    "\n",
    "    ylims = ax.get_ylim()\n",
    "\n",
    "    # plot annotation\n",
    "    leg = ax.legend(loc='upper right', fancybox=True, fontsize=8)\n",
    "    leg.get_frame().set_alpha(0.5)\n",
    "    ax.set_ylim([0, max(ylims)+2])'''\n",
    "\n",
    "    ax.set_xlabel('LD1', size=20)\n",
    "    #ax.set_title('Histogram #%s' %str(cnt+1), size=20)\n",
    "\n",
    "    # hide axis ticks\n",
    "    ax.tick_params(axis=\"both\", which=\"both\", bottom=\"off\", top=\"off\",  \n",
    "            labelbottom=\"on\", left=\"off\", right=\"off\", labelleft=\"on\", labelsize=20)\n",
    "\n",
    "    # remove axis spines\n",
    "    ax.spines[\"top\"].set_visible(False)  \n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines[\"bottom\"].set_visible(False)\n",
    "    ax.spines[\"left\"].set_visible(False)    \n",
    "\n",
    "    ax.set_ylabel('Count', size=20)\n",
    "    \n",
    "    \n",
    "    plt.legend(loc=\"upper right\", fontsize=20)\n",
    "    #fig.tight_layout() \n",
    "    #plt.annotate(str(add_on), xy=(0.02,0.95),xycoords='axes fraction', size=20)\n",
    "    #plt.annotate('Mean Accuracy = '+str(dec), xy=(0.02,0.9),xycoords='axes fraction', size=20)\n",
    "    #frame1 = plt.gca()\n",
    "    if run=='fg1_m_13':\n",
    "        plt.title('FG1M13')\n",
    "    if run=='fg3_m12':\n",
    "        plt.title('FG3M12')\n",
    "    if run=='fg3_m15':\n",
    "        plt.tilte('FG3M15')\n",
    "    plt.show()\n",
    "    print(coef)\n",
    "    #plt.savefig('../MaNGA_Papers/Paper_I/Marginalized_img_'+str(run)+'.pdf')\n",
    "    #plt.clf()\n",
    "    \n",
    "    '''Also, making those mountain plots for the imaging runs'''\n",
    "    \n",
    "    \n",
    "    \n",
    "    '''Now measure LD1 for every row and then plot that'''\n",
    "    import seaborn as sns\n",
    "    \n",
    "\n",
    "    n_params=7\n",
    "\n",
    "\n",
    "\n",
    "    #coef is how you get the eigvecs (doesn't matter what slope offset is)\n",
    "    #print('real eigvecs',(eigvec_sc.real))\n",
    "    #print(len(X_lda[:,0].real[y==2]))#[y == label]\n",
    "    xs=[]\n",
    "    LDA1=[]\n",
    "    if run=='fg3_m15':\n",
    "        myr=[320,340,360,400,420]\n",
    "    if run=='fg3_m_12':\n",
    "        myr=[170,180,185,190,195,205,210,220,225,230,240,250,260]\n",
    "        myr_non=[5,200]\n",
    "    if run=='fg3_m12':\n",
    "        myr=[5,10,20,30,40,60,80,100,120,140,160,170,180,185,190,195,205,210,220,225,230,240,250,260]\n",
    "        myr_non=[5,100,200]\n",
    "        myr_non=[5,10,20,30,100,200]\n",
    "    if run=='fg1_m13':\n",
    "        myr=[10,40,50,60,70,90,100,120,130,140,170,180,185,190,195,200,205,210,215,220,225,230,235,240,250,260,270,280,290,300,310,320,330,340,350]\n",
    "        myr_non=[5,10,100,200]\n",
    "    if run=='fg1_m_13':\n",
    "        myr=[40,195,210,215,220,225,230,235,240,250,260,270,280,290,300,310,320,330,340,350]\n",
    "        myr_non=[5,200]\n",
    "        \n",
    "    myr_non=[0,1,2,3,4,5,6]\n",
    "    my_lists = {key:[] for key in myr}\n",
    "    my_lists_none = {key:[] for key in myr_non}\n",
    "    myr_lists_none = {key:[] for key in myr_non}\n",
    "    \n",
    "    \n",
    "    my_lists_non = []\n",
    "    separations = {key:[] for key in myr}\n",
    "\n",
    "    \n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        if df[['class label']].values[i]==0:\n",
    "            my_lists_non.append(X_lda_sklearn[i][0])\n",
    "            my_lists_none[df[['Viewpoint']].values[i][0]].append(X_lda_sklearn[i][0])\n",
    "            myr_lists_none[df[['Viewpoint']].values[i][0]].append(df[['Myr']].values[i][0]/100)\n",
    "            continue\n",
    "        my_lists[df[['Myr']].values[i][0]].append(X_lda_sklearn[i][0])\n",
    "        separations[df[['Myr']].values[i][0]].append(df[['Sep']].values[i][0])\n",
    "        L=X_lda_sklearn[i][0]\n",
    "        #df[['Gini']].values[i][0]*coef[0][0]+df[['M20']].values[i][0]*coef[0][1]+df[['Concentration (C)']].values[i][0]*coef[0][2]+df[['Asymmetry (A)']].values[i][0]*coef[0][3]+df[['Clumpiness (S)']].values[i][0]*coef[0][4]+df[['Sersic N']].values[i][0]*coef[0][5]+df[['Shape Asymmetry']].values[i][0]*coef[0][6]\n",
    "        LDA1.append(L)\n",
    "        xs.append(df[['Myr']].values[i][0])\n",
    "    \n",
    "    \n",
    "    '''Make the beautiful list of colors'''\n",
    "    # These are the \"Tableau 20\" colors as RGB.    \n",
    "    tableau20 = [(31, 119, 180), (174, 199, 232), (255, 127, 14), (255, 187, 120),    \n",
    "                 (44, 160, 44), (152, 223, 138), (214, 39, 40), (255, 152, 150),    \n",
    "                 (148, 103, 189), (197, 176, 213), (140, 86, 75), (196, 156, 148),    \n",
    "                 (227, 119, 194), (247, 182, 210), (127, 127, 127), (199, 199, 199),    \n",
    "                 (188, 189, 34), (219, 219, 141), (23, 190, 207), (158, 218, 229)]    \n",
    "\n",
    "    # Scale the RGB values to the [0, 1] range, which is the format matplotlib accepts.    \n",
    "    for i in range(len(tableau20)):    \n",
    "        r, g, b = tableau20[i]    \n",
    "        tableau20[i] = (r / 255., g / 255., b / 255.)    \n",
    "    \n",
    "    mean_non=np.mean(my_lists_non)+np.std(my_lists_non)\n",
    "    means=[]\n",
    "    std=[]\n",
    "    separation_value=[]\n",
    "    plt.clf()\n",
    "    \n",
    "    for i in range(len(myr)):\n",
    "        means.append(np.mean(my_lists[myr[i]]))\n",
    "        std.append(np.std(my_lists[myr[i]]))\n",
    "        separation_value.append(np.mean(separations[myr[i]]))\n",
    "    for i in range(len(myr_non)):\n",
    "        print('viewpoint', i, 'color', tableau20[i])\n",
    "        print('xs',myr_lists_none[i])\n",
    "        print('ys',my_lists_none[i])\n",
    "        plt.scatter(myr_lists_none[i],my_lists_none[i], color=tableau20[i], label='Viewpoint '+str(i))\n",
    "    \n",
    "    \n",
    "    means=np.array(means)\n",
    "    std=np.array(std)\n",
    "    myr=np.array(myr)\n",
    "    \n",
    "    if run=='fg1_m13':\n",
    "        '''Try to replot with a dimensionless y axis with just means and std'''\n",
    "        rescale_y_mean=0#-1.7583e-01\n",
    "        rescale_y_pm=7.13475416061/2\n",
    "\n",
    "\n",
    "        new_means=np.array([(x-rescale_y_mean) for x in means])\n",
    "        #new_means=ma.masked_where(math.isnan(new_means),new_means)\n",
    "        #print('these are apparently the myrs', myr/100)\n",
    "        #print('these are the fills', new_means)\n",
    "        \n",
    "        plt.plot(myr/100, new_means, color=sns.xkcd_rgb[\"amber\"])\n",
    "        plt.fill_between(myr/100, (new_means-std), (new_means+std),alpha=.5, color=sns.xkcd_rgb[\"amber\"])\n",
    "        plt.xlabel(r'Merger Timeline [Gyr]', size=15)\n",
    "        plt.ylabel(r'Detection Sensitivity (LD1)', size=15)\n",
    "        #plt.axvline(x=220/100, color='black', ls='--')\n",
    "        plt.axvline(x=215/100, color='black', ls='--')\n",
    "        plt.axvline(x=280/100, color='black', ls='--')\n",
    "\n",
    "        '''width and height 3.17421463155 0.87713041243\n",
    "        pos [ -1.6779e+00   2.6963e-16]\n",
    "        width and height 5.56843021209 2.94988106646\n",
    "        pos [  2.7636e-01   1.2800e-16]'''\n",
    "\n",
    "\n",
    "        '''fg1_m_13:\n",
    "        width and height 1.98151143695 1.33924739273\n",
    "        pos [  1.4067e+00   6.6911e-16]\n",
    "        width and height 7.13475416061 2.66110273731\n",
    "        pos [ -1.7583e-01  -1.3878e-16]'''\n",
    "\n",
    "        plt.axhline(y=mean_non, color='black')\n",
    "        #plt.annotate(r'$\\mu_{\\mathrm{Merger}}$', xy=(0.02,0.53), xycoords='axes fraction', size=15)\n",
    "        #plt.title(str(run))\n",
    "        plt.annotate('FG1M13', xy=(0.02,1.02), xycoords='axes fraction', size=20)\n",
    "        plt.annotate('Early', xy=(0.25,0.95), xycoords='axes fraction', size=9)\n",
    "        plt.annotate('Late', xy=(0.63,0.95), xycoords='axes fraction',size=9)\n",
    "        plt.annotate('Post Coalescence', xy=(0.76,1.02), xycoords='axes fraction', size=9)\n",
    "\n",
    "    if run=='fg1_m_13':\n",
    "        '''Try to replot with a dimensionless y axis with just means and std'''\n",
    "        rescale_y_mean=0#-1.7583e-01\n",
    "        rescale_y_pm=7.13475416061/2\n",
    "\n",
    "\n",
    "        new_means=np.array([(x-rescale_y_mean) for x in means])\n",
    "\n",
    "        \n",
    "\n",
    "        plt.plot(myr/100, new_means, color=sns.xkcd_rgb[\"amber\"])\n",
    "        plt.fill_between(myr/100, (new_means-std), (new_means+std),alpha=.5, color=sns.xkcd_rgb[\"amber\"])\n",
    "        plt.xlabel(r'Merger Timeline [Gyr]', size=15)\n",
    "        plt.ylabel(r'Detection Sensitivity (LD1)', size=15)\n",
    "        #plt.axvline(x=220/100, color='black', ls='--')\n",
    "        plt.axvline(x=215/100, color='black', ls='--')\n",
    "        plt.axvline(x=280/100, color='black', ls='--')\n",
    "\n",
    "        '''width and height 3.17421463155 0.87713041243\n",
    "        pos [ -1.6779e+00   2.6963e-16]\n",
    "        width and height 5.56843021209 2.94988106646\n",
    "        pos [  2.7636e-01   1.2800e-16]'''\n",
    "\n",
    "\n",
    "        '''fg1_m_13:\n",
    "        width and height 1.98151143695 1.33924739273\n",
    "        pos [  1.4067e+00   6.6911e-16]\n",
    "        width and height 7.13475416061 2.66110273731\n",
    "        pos [ -1.7583e-01  -1.3878e-16]'''\n",
    "\n",
    "        ys_LD1=np.array([-1.7583e-01 for x in myr])\n",
    "        #plt.plot(myr/100,ys_LD1)\n",
    "        #plt.fill_between(myr/100, ys_LD1-0.588/2, ys_LD1+0.588/2,alpha=.5)\n",
    "        plt.axhline(y=mean_non, color='black')\n",
    "        #plt.annotate(r'$\\mu_{\\mathrm{Merger}}$', xy=(0.02,0.53), xycoords='axes fraction', size=15)\n",
    "        #plt.title(str(run))\n",
    "        plt.annotate('FG1M13', xy=(0.02,1.02), xycoords='axes fraction', size=20)\n",
    "        plt.annotate('Early', xy=(0.25,0.95), xycoords='axes fraction', size=9)\n",
    "        plt.annotate('Late', xy=(0.63,0.95), xycoords='axes fraction',size=9)\n",
    "        plt.annotate('Post Coalescence', xy=(0.76,1.02), xycoords='axes fraction', size=9)\n",
    "    if run=='fg3_m_12':\n",
    "        '''Try to replot with a dimensionless y axis with just means and std'''\n",
    "        rescale_y_mean=0#2.7636e-01\n",
    "        rescale_y_pm=5.5684302120/2\n",
    "\n",
    "\n",
    "        new_means=np.array([(x-rescale_y_mean) for x in means])\n",
    "\n",
    "        \n",
    "\n",
    "        plt.plot(myr/100, new_means, color='red')\n",
    "        plt.fill_between(myr/100, (new_means-std), (new_means+std),alpha=.5, color='red')\n",
    "        plt.xlabel(r'Merger Timeline [Gyr]', size=15)\n",
    "        plt.ylabel(r'Detection Sensitivity (LD1)', size=15)\n",
    "        plt.axvline(x=220/100, color='black', ls='--')\n",
    "        plt.axvline(x=180/100, color='black', ls='--')\n",
    "\n",
    "\n",
    "\n",
    "        '''width and height 3.17421463155 0.87713041243\n",
    "        pos [ -1.6779e+00   2.6963e-16]\n",
    "        width and height 5.56843021209 2.94988106646\n",
    "        pos [  2.7636e-01   1.2800e-16]'''\n",
    "\n",
    "\n",
    "        #plt.axhline(y=0, color='black')\n",
    "\n",
    "        #plt.annotate(r'$\\mu_{\\mathrm{Merger}}$', xy=(0.15,0.53), xycoords='axes fraction', size=15)\n",
    "        #plt.title(str(run))\n",
    "        plt.axhline(y=mean_non, color='black')\n",
    "        plt.annotate('FG3M12', xy=(0.02,1.02), xycoords='axes fraction', size=20)\n",
    "        plt.annotate('Early', xy=(0.03,0.95), xycoords='axes fraction', size=9)\n",
    "        plt.annotate('Late', xy=(0.3,0.95), xycoords='axes fraction',size=9)\n",
    "        plt.annotate('Post Coalescence', xy=(0.65,0.95), xycoords='axes fraction', size=9)\n",
    "    if run=='fg3_m12':\n",
    "    \n",
    "        '''Try to replot with a dimensionless y axis with just means and std'''\n",
    "        rescale_y_mean=0#2.7636e-01\n",
    "        rescale_y_pm=5.5684302120/2\n",
    "\n",
    "\n",
    "        new_means=np.array([(x-rescale_y_mean) for x in means])\n",
    "\n",
    "        \n",
    "\n",
    "        plt.plot(myr/100, new_means, color='red')\n",
    "        plt.fill_between(myr/100, (new_means-std), (new_means+std),alpha=.5, color='red')\n",
    "        plt.xlabel(r'Merger Timeline [Gyr]', size=15)\n",
    "        plt.ylabel(r'Detection Sensitivity (LD1)', size=15)\n",
    "        plt.axvline(x=220/100, color='black', ls='--')\n",
    "        plt.axvline(x=180/100, color='black', ls='--')\n",
    "\n",
    "\n",
    "\n",
    "        '''width and height 3.17421463155 0.87713041243\n",
    "        pos [ -1.6779e+00   2.6963e-16]\n",
    "        width and height 5.56843021209 2.94988106646\n",
    "        pos [  2.7636e-01   1.2800e-16]'''\n",
    "\n",
    "\n",
    "        #plt.axhline(y=0, color='black')\n",
    "\n",
    "        #plt.annotate(r'$\\mu_{\\mathrm{Merger}}$', xy=(0.15,0.53), xycoords='axes fraction', size=15)\n",
    "        #plt.title(str(run))\n",
    "        plt.axhline(y=mean_non, color='black')\n",
    "        plt.annotate('FG3M12', xy=(0.02,1.02), xycoords='axes fraction', size=20)\n",
    "        plt.annotate('Early', xy=(0.63,0.97), xycoords='axes fraction', size=9)\n",
    "        plt.annotate('Late', xy=(0.7,0.97), xycoords='axes fraction',size=9)\n",
    "        plt.annotate('Post Coalescence', xy=(0.85,1.01), xycoords='axes fraction', size=9)\n",
    "    #plt.ylim([-1,1])\n",
    "    #plt.xlim([min(myr)/100,max(myr)/100])\n",
    "    #plt.xlim([0,])\n",
    "    frame1 = plt.gca()\n",
    "    #frame1.axes.xaxis.set_ticklabels([])\n",
    "    frame1.axes.yaxis.set_ticklabels([])\n",
    "    #frame1.axes.yaxis.set_ticks([])\n",
    "    plt.legend()\n",
    "    plt.savefig('../MaNGA_Papers/Paper_I/Mountain_plot_imaging_priors_view_colors_'+str(run)+'.pdf')\n",
    "\n",
    "\n",
    "\n",
    "    '''Try to replot with a dimensionless y axis with just means and std'''\n",
    "    \n",
    "    \n",
    "#    savefig('../MaNGA_Papers/Paper_I/Bayesian_Hist_'+str(run)+'.pdf')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prior_1 [0.   0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1  0.11 0.12 0.13\n",
      " 0.14 0.15 0.16 0.17 0.18 0.19 0.2  0.21 0.22 0.23 0.24 0.25 0.26 0.27\n",
      " 0.28 0.29 0.3  0.31 0.32 0.33 0.34 0.35 0.36 0.37 0.38 0.39 0.4  0.41\n",
      " 0.42 0.43 0.44 0.45 0.46 0.47 0.48 0.49 0.5  0.51 0.52 0.53 0.54 0.55\n",
      " 0.56 0.57 0.58 0.59 0.6  0.61 0.62 0.63 0.64 0.65 0.66 0.67 0.68 0.69\n",
      " 0.7  0.71 0.72 0.73 0.74 0.75 0.76 0.77 0.78 0.79 0.8  0.81 0.82 0.83\n",
      " 0.84 0.85 0.86 0.87 0.88 0.89 0.9  0.91 0.92 0.93 0.94 0.95 0.96 0.97\n",
      " 0.98 0.99 1.  ]\n",
      "run fg3_m12\n",
      "[[ 0.   0. ]\n",
      " [ 6.1 13.1]]\n",
      "0.0 13.1\n",
      "run fg3_m12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/beckynevin/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:38: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/Users/beckynevin/anaconda/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:401: RuntimeWarning: invalid value encountered in true_divide\n",
      "  S**2))[:self._max_components]\n",
      "/Users/beckynevin/anaconda/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:406: RuntimeWarning: divide by zero encountered in log\n",
      "  np.log(self.priors_))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.6  0.2]\n",
      " [ 0.5 12.9]]\n",
      "5.6 12.9\n",
      "run fg3_m12\n",
      "[[ 5.6  0.3]\n",
      " [ 0.5 12.8]]\n",
      "5.6 12.8\n",
      "run fg3_m12\n",
      "[[ 5.7  0.4]\n",
      " [ 0.4 12.7]]\n",
      "5.7 12.7\n",
      "run fg3_m12\n",
      "[[ 5.7  0.4]\n",
      " [ 0.4 12.7]]\n",
      "5.7 12.7\n",
      "run fg3_m12\n",
      "[[ 5.8  0.4]\n",
      " [ 0.3 12.7]]\n",
      "5.8 12.7\n",
      "run fg3_m12\n",
      "[[ 5.8  0.4]\n",
      " [ 0.3 12.7]]\n",
      "5.8 12.7\n",
      "run fg3_m12\n",
      "[[ 5.9  0.4]\n",
      " [ 0.2 12.7]]\n",
      "5.9 12.7\n",
      "run fg3_m12\n",
      "[[ 5.9  0.4]\n",
      " [ 0.2 12.7]]\n",
      "5.9 12.7\n",
      "run fg3_m12\n",
      "[[ 5.9  0.4]\n",
      " [ 0.2 12.7]]\n",
      "5.9 12.7\n",
      "run fg3_m12\n",
      "[[ 5.9  0.4]\n",
      " [ 0.2 12.7]]\n",
      "5.9 12.7\n",
      "run fg3_m12\n",
      "[[ 5.9  0.4]\n",
      " [ 0.2 12.7]]\n",
      "5.9 12.7\n",
      "run fg3_m12\n",
      "[[ 5.9  0.4]\n",
      " [ 0.2 12.7]]\n",
      "5.9 12.7\n",
      "run fg3_m12\n",
      "[[ 5.9  0.4]\n",
      " [ 0.2 12.7]]\n",
      "5.9 12.7\n",
      "run fg3_m12\n",
      "[[ 5.9  0.5]\n",
      " [ 0.2 12.6]]\n",
      "5.9 12.6\n",
      "run fg3_m12\n",
      "[[ 5.9  0.5]\n",
      " [ 0.2 12.6]]\n",
      "5.9 12.6\n",
      "run fg3_m12\n",
      "[[ 5.9  0.6]\n",
      " [ 0.2 12.5]]\n",
      "5.9 12.5\n",
      "run fg3_m12\n",
      "[[ 5.9  0.6]\n",
      " [ 0.2 12.5]]\n",
      "5.9 12.5\n",
      "run fg3_m12\n",
      "[[ 5.9  0.6]\n",
      " [ 0.2 12.5]]\n",
      "5.9 12.5\n",
      "run fg3_m12\n",
      "[[ 5.9  0.6]\n",
      " [ 0.2 12.5]]\n",
      "5.9 12.5\n",
      "run fg3_m12\n",
      "[[ 5.9  0.6]\n",
      " [ 0.2 12.5]]\n",
      "5.9 12.5\n",
      "run fg3_m12\n",
      "[[ 5.9  0.6]\n",
      " [ 0.2 12.5]]\n",
      "5.9 12.5\n",
      "run fg3_m12\n",
      "[[ 5.9  0.6]\n",
      " [ 0.2 12.5]]\n",
      "5.9 12.5\n",
      "run fg3_m12\n",
      "[[ 5.9  0.6]\n",
      " [ 0.2 12.5]]\n",
      "5.9 12.5\n",
      "run fg3_m12\n",
      "[[ 5.9  0.6]\n",
      " [ 0.2 12.5]]\n",
      "5.9 12.5\n",
      "run fg3_m12\n",
      "[[ 5.9  0.6]\n",
      " [ 0.2 12.5]]\n",
      "5.9 12.5\n",
      "run fg3_m12\n",
      "[[ 5.9  0.6]\n",
      " [ 0.2 12.5]]\n",
      "5.9 12.5\n",
      "run fg3_m12\n",
      "[[ 5.9  0.6]\n",
      " [ 0.2 12.5]]\n",
      "5.9 12.5\n",
      "run fg3_m12\n",
      "[[ 5.9  0.6]\n",
      " [ 0.2 12.5]]\n",
      "5.9 12.5\n",
      "run fg3_m12\n",
      "[[ 5.9  0.6]\n",
      " [ 0.2 12.5]]\n",
      "5.9 12.5\n",
      "run fg3_m12\n",
      "[[ 5.9  0.6]\n",
      " [ 0.2 12.5]]\n",
      "5.9 12.5\n",
      "run fg3_m12\n",
      "[[ 5.9  0.6]\n",
      " [ 0.2 12.5]]\n",
      "5.9 12.5\n",
      "run fg3_m12\n",
      "[[ 5.9  0.6]\n",
      " [ 0.2 12.5]]\n",
      "5.9 12.5\n",
      "run fg3_m12\n",
      "[[ 5.9  0.7]\n",
      " [ 0.2 12.4]]\n",
      "5.9 12.4\n",
      "run fg3_m12\n",
      "[[ 5.9  0.7]\n",
      " [ 0.2 12.4]]\n",
      "5.9 12.4\n",
      "run fg3_m12\n",
      "[[ 5.9  0.7]\n",
      " [ 0.2 12.4]]\n",
      "5.9 12.4\n",
      "run fg3_m12\n",
      "[[ 5.9  0.7]\n",
      " [ 0.2 12.4]]\n",
      "5.9 12.4\n",
      "run fg3_m12\n",
      "[[ 5.9  0.7]\n",
      " [ 0.2 12.4]]\n",
      "5.9 12.4\n",
      "run fg3_m12\n",
      "[[ 5.9  0.7]\n",
      " [ 0.2 12.4]]\n",
      "5.9 12.4\n",
      "run fg3_m12\n",
      "[[ 5.9  0.7]\n",
      " [ 0.2 12.4]]\n",
      "5.9 12.4\n",
      "run fg3_m12\n",
      "[[ 5.9  0.7]\n",
      " [ 0.2 12.4]]\n",
      "5.9 12.4\n",
      "run fg3_m12\n",
      "[[ 5.9  0.7]\n",
      " [ 0.2 12.4]]\n",
      "5.9 12.4\n",
      "run fg3_m12\n",
      "[[ 5.9  0.7]\n",
      " [ 0.2 12.4]]\n",
      "5.9 12.4\n",
      "run fg3_m12\n",
      "[[ 5.9  0.7]\n",
      " [ 0.2 12.4]]\n",
      "5.9 12.4\n",
      "run fg3_m12\n",
      "[[ 5.9  0.7]\n",
      " [ 0.2 12.4]]\n",
      "5.9 12.4\n",
      "run fg3_m12\n",
      "[[ 5.9  0.7]\n",
      " [ 0.2 12.4]]\n",
      "5.9 12.4\n",
      "run fg3_m12\n",
      "[[ 5.9  0.7]\n",
      " [ 0.2 12.4]]\n",
      "5.9 12.4\n",
      "run fg3_m12\n",
      "[[ 5.9  0.7]\n",
      " [ 0.2 12.4]]\n",
      "5.9 12.4\n",
      "run fg3_m12\n",
      "[[ 5.9  0.7]\n",
      " [ 0.2 12.4]]\n",
      "5.9 12.4\n",
      "run fg3_m12\n",
      "[[ 5.9  0.7]\n",
      " [ 0.2 12.4]]\n",
      "5.9 12.4\n",
      "run fg3_m12\n",
      "[[ 5.9  0.7]\n",
      " [ 0.2 12.4]]\n",
      "5.9 12.4\n",
      "run fg3_m12\n",
      "[[ 5.9  0.7]\n",
      " [ 0.2 12.4]]\n",
      "5.9 12.4\n",
      "run fg3_m12\n",
      "[[ 5.9  0.9]\n",
      " [ 0.2 12.2]]\n",
      "5.9 12.2\n",
      "run fg3_m12\n",
      "[[ 5.9  0.9]\n",
      " [ 0.2 12.2]]\n",
      "5.9 12.2\n",
      "run fg3_m12\n",
      "[[ 5.9  0.9]\n",
      " [ 0.2 12.2]]\n",
      "5.9 12.2\n",
      "run fg3_m12\n",
      "[[ 5.9  0.9]\n",
      " [ 0.2 12.2]]\n",
      "5.9 12.2\n",
      "run fg3_m12\n",
      "[[ 5.9  0.9]\n",
      " [ 0.2 12.2]]\n",
      "5.9 12.2\n",
      "run fg3_m12\n",
      "[[ 5.9  0.9]\n",
      " [ 0.2 12.2]]\n",
      "5.9 12.2\n",
      "run fg3_m12\n",
      "[[ 5.9  0.9]\n",
      " [ 0.2 12.2]]\n",
      "5.9 12.2\n",
      "run fg3_m12\n",
      "[[ 5.9  0.9]\n",
      " [ 0.2 12.2]]\n",
      "5.9 12.2\n",
      "run fg3_m12\n",
      "[[ 5.9  0.9]\n",
      " [ 0.2 12.2]]\n",
      "5.9 12.2\n",
      "run fg3_m12\n",
      "[[ 5.9  0.9]\n",
      " [ 0.2 12.2]]\n",
      "5.9 12.2\n",
      "run fg3_m12\n",
      "[[ 5.9  0.9]\n",
      " [ 0.2 12.2]]\n",
      "5.9 12.2\n",
      "run fg3_m12\n",
      "[[ 5.9  0.9]\n",
      " [ 0.2 12.2]]\n",
      "5.9 12.2\n",
      "run fg3_m12\n",
      "[[ 5.9  0.9]\n",
      " [ 0.2 12.2]]\n",
      "5.9 12.2\n",
      "run fg3_m12\n",
      "[[ 5.9  0.9]\n",
      " [ 0.2 12.2]]\n",
      "5.9 12.2\n",
      "run fg3_m12\n",
      "[[ 5.9  0.9]\n",
      " [ 0.2 12.2]]\n",
      "5.9 12.2\n",
      "run fg3_m12\n",
      "[[ 5.9  0.9]\n",
      " [ 0.2 12.2]]\n",
      "5.9 12.2\n",
      "run fg3_m12\n",
      "[[ 5.9  0.9]\n",
      " [ 0.2 12.2]]\n",
      "5.9 12.2\n",
      "run fg3_m12\n",
      "[[ 6.   0.9]\n",
      " [ 0.1 12.2]]\n",
      "6.0 12.2\n",
      "run fg3_m12\n",
      "[[ 6.   0.9]\n",
      " [ 0.1 12.2]]\n",
      "6.0 12.2\n",
      "run fg3_m12\n",
      "[[ 6.   0.9]\n",
      " [ 0.1 12.2]]\n",
      "6.0 12.2\n",
      "run fg3_m12\n",
      "[[ 6.   0.9]\n",
      " [ 0.1 12.2]]\n",
      "6.0 12.2\n",
      "run fg3_m12\n",
      "[[ 6.   0.9]\n",
      " [ 0.1 12.2]]\n",
      "6.0 12.2\n",
      "run fg3_m12\n",
      "[[ 6.   0.9]\n",
      " [ 0.1 12.2]]\n",
      "6.0 12.2\n",
      "run fg3_m12\n",
      "[[ 6.   0.9]\n",
      " [ 0.1 12.2]]\n",
      "6.0 12.2\n",
      "run fg3_m12\n",
      "[[ 6.   0.9]\n",
      " [ 0.1 12.2]]\n",
      "6.0 12.2\n",
      "run fg3_m12\n",
      "[[ 6.   0.9]\n",
      " [ 0.1 12.2]]\n",
      "6.0 12.2\n",
      "run fg3_m12\n",
      "[[ 6.   0.9]\n",
      " [ 0.1 12.2]]\n",
      "6.0 12.2\n",
      "run fg3_m12\n",
      "[[ 6.   0.9]\n",
      " [ 0.1 12.2]]\n",
      "6.0 12.2\n",
      "run fg3_m12\n",
      "[[ 6.   0.9]\n",
      " [ 0.1 12.2]]\n",
      "6.0 12.2\n",
      "run fg3_m12\n",
      "[[ 6.   0.9]\n",
      " [ 0.1 12.2]]\n",
      "6.0 12.2\n",
      "run fg3_m12\n",
      "[[ 6.   0.9]\n",
      " [ 0.1 12.2]]\n",
      "6.0 12.2\n",
      "run fg3_m12\n",
      "[[ 6.   1. ]\n",
      " [ 0.1 12.1]]\n",
      "6.0 12.1\n",
      "run fg3_m12\n",
      "[[ 6.   1. ]\n",
      " [ 0.1 12.1]]\n",
      "6.0 12.1\n",
      "run fg3_m12\n",
      "[[ 6.   1. ]\n",
      " [ 0.1 12.1]]\n",
      "6.0 12.1\n",
      "run fg3_m12\n",
      "[[ 6.   1. ]\n",
      " [ 0.1 12.1]]\n",
      "6.0 12.1\n",
      "run fg3_m12\n",
      "[[ 6.   1. ]\n",
      " [ 0.1 12.1]]\n",
      "6.0 12.1\n",
      "run fg3_m12\n",
      "[[ 6.   1. ]\n",
      " [ 0.1 12.1]]\n",
      "6.0 12.1\n",
      "run fg3_m12\n",
      "[[ 6.   1. ]\n",
      " [ 0.1 12.1]]\n",
      "6.0 12.1\n",
      "run fg3_m12\n",
      "[[ 6.   1.2]\n",
      " [ 0.1 11.9]]\n",
      "6.0 11.9\n",
      "run fg3_m12\n",
      "[[ 6.   1.3]\n",
      " [ 0.1 11.8]]\n",
      "6.0 11.8\n",
      "run fg3_m12\n",
      "[[ 6.   1.3]\n",
      " [ 0.1 11.8]]\n",
      "6.0 11.8\n",
      "run fg3_m12\n",
      "[[ 6.   1.4]\n",
      " [ 0.1 11.7]]\n",
      "6.0 11.7\n",
      "run fg3_m12\n",
      "[[ 6.   1.5]\n",
      " [ 0.1 11.6]]\n",
      "6.0 11.6\n",
      "run fg3_m12\n",
      "[[ 6.   1.6]\n",
      " [ 0.1 11.5]]\n",
      "6.0 11.5\n",
      "run fg3_m12\n",
      "[[ 6.   1.7]\n",
      " [ 0.1 11.4]]\n",
      "6.0 11.4\n",
      "run fg3_m12\n",
      "[[ 6.   1.7]\n",
      " [ 0.1 11.4]]\n",
      "6.0 11.4\n",
      "run fg3_m12\n",
      "[[ 6.   2.4]\n",
      " [ 0.1 10.7]]\n",
      "6.0 10.7\n",
      "run fg3_m12\n",
      "[[ 6.   2.6]\n",
      " [ 0.1 10.5]]\n",
      "6.0 10.5\n",
      "run fg3_m12\n",
      "[[ 6.1 13.1]\n",
      " [ 0.   0. ]]\n",
      "6.1 0.0\n"
     ]
    }
   ],
   "source": [
    "'''This section is for testing if the LDA is sensitive to priors'''\n",
    "\n",
    "\n",
    "\n",
    "prior_1=np.linspace(0,1,101)\n",
    "print('prior_1', prior_1)\n",
    "#this is fraction nonmerg\n",
    "\n",
    "acc_12=[]\n",
    "\n",
    "\n",
    "for i in range(len(prior_1)):\n",
    "   \n",
    "    add_on='fg3_m12'#'fg3_m12'\n",
    "    print('run', add_on)\n",
    "\n",
    "\n",
    "    run=add_on\n",
    "    df = pd.io.parsers.read_table(\n",
    "        filepath_or_buffer='LDA_img_ratio_statmorph_'+str(run)+'.txt',#'_view_all.txt',\n",
    "        header=[0],\n",
    "        sep='\\t'\n",
    "        )#,skiprows=10,nrows=10\n",
    "    df.columns = [l for i,l in sorted(feature_dict.items())] + ['Shape Asymmetry']\n",
    "    df.dropna(how=\"all\", inplace=True) # to drop the empty line at file-end\n",
    "    \n",
    "    for j in range(len(df)):\n",
    "        if df[['Myr']].values[j][0]<40 and df[['Sep']].values[j][0]==0.0 and df[['# Bulges']].values[j][0]==1:#df[['Myr']].values[i][0]\n",
    "            \n",
    "            \n",
    "            #I use this part to check if there is any separation at these points in time\n",
    "            #Or if there are more than two bulges\n",
    "            #print(df[['class label','Myr','Viewpoint','# Bulges', 'Sep']].values[j])\n",
    "            \n",
    "            #Then, you can optionally change the class values of all of these viewpoints\n",
    "            \n",
    "            #.set_value(index, col, value, \n",
    "            df.set_value(j,'class label',0.0)\n",
    "    \n",
    "    \n",
    "\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "    X = df[['Gini','M20','Concentration (C)', 'Asymmetry (A)', 'Clumpiness (S)', 'Sersic N', 'Shape Asymmetry']].values\n",
    "    \n",
    "    from sklearn import preprocessing\n",
    "\n",
    "    \n",
    "\n",
    "    std_scale = preprocessing.StandardScaler().fit(X)\n",
    "    X = std_scale.transform(X)\n",
    "    \n",
    "    n_params=7\n",
    "\n",
    "\n",
    "    y = df['class label'].values\n",
    "    \n",
    "\n",
    "    enc = LabelEncoder()\n",
    "    label_encoder = enc.fit(y)\n",
    "    y = label_encoder.transform(y) + 1\n",
    "\n",
    "\n",
    "    label_dict = {1: 'NonMerger', 2: 'Merger'}\n",
    "    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "    # LDA\n",
    "    '''sklearn_lda = LDA(priors=[prior_1[i],1-prior_1[i]])\n",
    "    X_lda_sklearn = sklearn_lda.fit_transform(X, y)\n",
    "    dec = sklearn_lda.score(X,y)\n",
    "    prob = sklearn_lda.predict_proba(X)\n",
    "    \n",
    "    coef = sklearn_lda.coef_\n",
    "    inter = sklearn_lda.intercept_\n",
    "    class_label = sklearn_lda.classes_'''\n",
    "    \n",
    "    '''New method'''\n",
    "    from sklearn.model_selection import KFold\n",
    "    kf = KFold(n_splits=10, random_state=True, shuffle=True)#len(X))\n",
    "    \n",
    "    \n",
    "    kf.get_n_splits(X)\n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "    confusion_master=[]\n",
    "    count=0\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        \n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        sklearn_lda = LDA(priors=[prior_1[i],1-prior_1[i]], store_covariance=True)#store_covariance=False\n",
    "    \n",
    "    \n",
    "    \n",
    "        X_lda_sklearn = sklearn_lda.fit_transform(X_train, y_train)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        pred =sklearn_lda.predict(X_test)\n",
    "        \n",
    "        \n",
    "        confusion_master.append(confusion_matrix(pred,y_test))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    print(np.mean(confusion_master, axis=0))\n",
    "    new_conf=np.mean(confusion_master, axis=0)\n",
    "    print(new_conf[0][0], new_conf[1][1])\n",
    "    accuracy=(new_conf[0][0]+new_conf[1][1])/(np.sum(new_conf))\n",
    "    \n",
    "    \n",
    "   \n",
    "    #print('mean accuracy',dec)#mean accuracy on the given test data and labels.\n",
    "    acc_12.append(accuracy)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.plot(prior_1, acc_13,  label='q0.333_fg0.1')\n",
    "plt.plot(prior_1, acc_12,  label='q0.5_fg0.3')\n",
    "plt.plot(prior_1, acc_15,  label='q0.2_fg0.3_BT0.2')\n",
    "plt.xlabel(r'$f_{\\mathrm{nonmerg}}$', size=15)\n",
    "plt.ylabel('LDA Accuracy', size=15)\n",
    "plt.axvline(x=0.9, ls='--', color='black')\n",
    "plt.legend()\n",
    "plt.xlim([0,1])\n",
    "#plt.show()\n",
    "plt.savefig('../MaNGA_Papers/Paper_I/insensitive_to_priors.pdf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q0.5_fg0.3 vs q0.5_fg0.3 0.0\n",
      "q0.5_fg0.3 vs q0.333_fg0.1 35.00562395799082\n",
      "q0.5_fg0.3 vs q0.2_fg0.3 48.159754311711765\n",
      "q0.333_fg0.1 vs q0.5_fg0.3 35.00562395799082\n",
      "q0.333_fg0.1 vs q0.333_fg0.1 0.0\n",
      "q0.333_fg0.1 vs q0.2_fg0.3 22.585970746417956\n",
      "q0.2_fg0.3 vs q0.5_fg0.3 48.159754311711765\n",
      "q0.2_fg0.3 vs q0.333_fg0.1 22.585970746417956\n",
      "Domain error q0.2_fg0.3 vs q0.2_fg0.3\n",
      "q0.2_fg0.3 vs q0.2_fg0.3 22.585970746417956\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "fg3_m12_eig=[4.3,-0.7,1.4,3.2,-2.2,2.5,1.8]\n",
    "fg1_m13_eig=[3.0,-1.7,3.9,3.7,-2.7,-0.2,1.1]\n",
    "fg3_m15_eig=[14.0, -1.0, 24.4, 9.7, -14.0, -6.0, 6.2]\n",
    "\n",
    "list_names=[fg3_m12_eig,fg1_m13_eig,fg3_m15_eig]\n",
    "act_names=['q0.5_fg0.3','q0.333_fg0.1', 'q0.2_fg0.3']\n",
    "for x in range(len(list_names)):\n",
    "    for y in range(len(list_names)):\n",
    "        try:\n",
    "            exp=math.degrees(math.acos(np.dot(list_names[x],list_names[y])/(np.linalg.norm(list_names[x])*np.linalg.norm(list_names[y]))))\n",
    "        except ValueError:\n",
    "            print('Domain error', act_names[x], 'vs', act_names[y])\n",
    "        if exp <90:\n",
    "            expnow=exp\n",
    "        else:\n",
    "            expnow=abs(180-exp)\n",
    "        print(act_names[x], 'vs', act_names[y], expnow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.24905461]\n",
      " [-1.68091976]\n",
      " [ 6.11884828]\n",
      " [ 5.47198309]\n",
      " [ 4.60309771]\n",
      " [ 4.87954727]\n",
      " [ 4.17413156]\n",
      " [ 2.41719754]\n",
      " [ 2.97902972]\n",
      " [ 3.25976892]\n",
      " [ 3.36865114]\n",
      " [ 3.5545208 ]\n",
      " [ 3.75685733]\n",
      " [ 3.84589582]\n",
      " [-1.54500051]\n",
      " [-1.42974055]\n",
      " [ 4.22642622]\n",
      " [ 3.33484153]\n",
      " [ 3.81751928]\n",
      " [ 3.06943415]\n",
      " [ 3.27857911]\n",
      " [ 2.3236804 ]\n",
      " [ 4.03394623]\n",
      " [ 4.21368238]\n",
      " [ 3.49088418]\n",
      " [ 3.57496975]\n",
      " [ 3.95586172]\n",
      " [ 3.37118699]\n",
      " [-1.58020943]\n",
      " [-0.89848461]\n",
      " [ 4.14912498]\n",
      " [ 4.82284989]\n",
      " [ 3.81352123]\n",
      " [ 3.91659071]\n",
      " [ 3.85951445]\n",
      " [ 2.34015896]\n",
      " [ 2.63493089]\n",
      " [ 3.07472385]\n",
      " [ 3.9168378 ]\n",
      " [ 3.29674971]\n",
      " [ 3.6968283 ]\n",
      " [ 2.42648068]\n",
      " [-1.70062429]\n",
      " [-1.08279531]\n",
      " [ 4.64566041]\n",
      " [ 2.53592523]\n",
      " [ 3.69385845]\n",
      " [ 4.23612936]\n",
      " [ 4.19900727]\n",
      " [ 2.78530766]\n",
      " [ 4.36577064]\n",
      " [ 4.67723876]\n",
      " [ 4.62902936]\n",
      " [ 2.9028692 ]\n",
      " [ 3.9237619 ]\n",
      " [ 2.82556094]\n",
      " [-0.6409273 ]\n",
      " [-0.12130954]\n",
      " [ 5.06461006]\n",
      " [ 4.92248169]\n",
      " [ 2.81614121]\n",
      " [ 5.44111939]\n",
      " [ 3.62730409]\n",
      " [ 4.05635161]\n",
      " [ 3.83527829]\n",
      " [ 4.03960082]\n",
      " [ 3.73802247]\n",
      " [ 2.96499894]\n",
      " [ 2.22000116]\n",
      " [ 2.32899665]\n",
      " [-0.55000439]\n",
      " [ 0.44774166]\n",
      " [ 3.6365453 ]\n",
      " [ 4.26161378]\n",
      " [ 6.94232498]\n",
      " [ 5.45799833]\n",
      " [ 2.0121493 ]\n",
      " [ 1.65244679]\n",
      " [ 2.74577448]\n",
      " [ 2.16668699]\n",
      " [ 2.30086837]\n",
      " [ 1.57605309]\n",
      " [ 1.79350285]\n",
      " [ 2.41279288]\n",
      " [-0.63209532]\n",
      " [ 0.25435787]\n",
      " [ 4.49372681]\n",
      " [ 3.72761484]\n",
      " [ 2.83895246]\n",
      " [ 3.36051157]\n",
      " [ 1.82887468]\n",
      " [ 3.07686672]\n",
      " [ 3.7596658 ]\n",
      " [ 3.14380381]\n",
      " [ 3.7143936 ]\n",
      " [ 3.290735  ]\n",
      " [ 2.53883788]\n",
      " [ 1.54096777]]\n",
      "0.979591836735\n",
      "[[-25.68071472   0.03452496   5.58761818  31.85126869 -25.11539707\n",
      "    2.12714878   2.84596643  -5.44687753  16.13223911]]\n",
      "[-10.32850329]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "# LDA\n",
    "sklearn_lda = LDA(priors=[0.8,0.2])\n",
    "X_lda_sklearn = sklearn_lda.fit_transform(X, y)\n",
    "dec = sklearn_lda.score(X,y)\n",
    "print(X_lda_sklearn)\n",
    "coef = sklearn_lda.coef_\n",
    "inter = sklearn_lda.intercept_\n",
    "print(dec)\n",
    "print(coef)\n",
    "print(inter)\n",
    "\n",
    "#plot_scikit_lda(X_lda_sklearn, title='Default LDA via scikit-learn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_scikit_lda(X, title):\n",
    "\n",
    "    ax = plt.subplot(111)\n",
    "    for label,marker,color in zip(\n",
    "        range(1,4),('^', 's', 'o'),('blue', 'red', 'green')):\n",
    "\n",
    "        plt.scatter(x=X[:,0][y == label],\n",
    "                    y=0 * -1, # flip the figure\n",
    "                    marker=marker,\n",
    "                    color=color,\n",
    "                    alpha=0.5,\n",
    "                    label=label_dict[label])\n",
    "\n",
    "    plt.xlabel('LD1')\n",
    "    plt.ylabel('LD2')\n",
    "\n",
    "    leg = plt.legend(loc='upper right', fancybox=True)\n",
    "    leg.get_frame().set_alpha(0.5)\n",
    "    plt.title(title)\n",
    "\n",
    "    # hide axis ticks\n",
    "    plt.tick_params(axis=\"both\", which=\"both\", bottom=\"off\", top=\"off\",  \n",
    "            labelbottom=\"on\", left=\"off\", right=\"off\", labelleft=\"on\")\n",
    "\n",
    "    # remove axis spines\n",
    "    ax.spines[\"top\"].set_visible(False)  \n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines[\"bottom\"].set_visible(False)\n",
    "    ax.spines[\"left\"].set_visible(False)    \n",
    "\n",
    "    plt.grid()\n",
    "    plt.tight_layout\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run fg3_m12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/beckynevin/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:88: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~Coefficients and Intercepts~~~\n",
      "[[ 4.23953147 -0.67944072  1.39674296  3.13455911 -2.13520944  2.44740597\n",
      "   1.7828445 ]] [0.1398731]\n",
      "run fg3_m12\n",
      "~~~Coefficients and Intercepts~~~\n",
      "[[ 4.23953147 -0.67944072  1.39674296  3.13455911 -2.13520944  2.44740597\n",
      "   1.7828445 ]] [1.15680735]\n",
      "run fg3_m12\n",
      "~~~Coefficients and Intercepts~~~\n",
      "[[ 4.23953147 -0.67944072  1.39674296  3.13455911 -2.13520944  2.44740597\n",
      "   1.7828445 ]] [5.64294372]\n",
      "run fg3_m12\n",
      "~~~Coefficients and Intercepts~~~\n",
      "[[ 4.23953147 -0.67944072  1.39674296  3.13455911 -2.13520944  2.44740597\n",
      "   1.7828445 ]] [-4.01534637]\n",
      "run fg3_m12\n",
      "~~~Coefficients and Intercepts~~~\n",
      "[[ 4.23953147 -0.67944072  1.39674296  3.13455911 -2.13520944  2.44740597\n",
      "   1.7828445 ]] [9.79816319]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This is to test how sensitive or insensitive the process is to the priors\n",
    "Priors vary:\n",
    "Lotz2011 f_merg = 15%\n",
    "\n",
    "'''\n",
    "\n",
    "'''\n",
    "~~~\n",
    "Now just for the imaging part of it!\n",
    "~~~\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_mean_and_CI(mean, lb, ub, color_mean=None, color_shading=None):\n",
    "        # plot the shaded range of the confidence intervals\n",
    "        plt.fill_between(range(mean.shape[0]), ub, lb,\n",
    "                         color=color_shading, alpha=.5)\n",
    "        # plot the mean on top\n",
    "        plt.plot(mean, color_mean)\n",
    "\n",
    "\n",
    "feature_dict = {i:label for i,label in zip(\n",
    "                range(14),\n",
    "                  ('Counter',\n",
    "                  'Image',\n",
    "                  'class label',\n",
    "                  'Myr',\n",
    "                  'Viewpoint',\n",
    "                '# Bulges',\n",
    "                   'Sep',\n",
    "                   'Flux Ratio',\n",
    "                  'Gini',\n",
    "                  'M20',\n",
    "                  'Concentration (C)',\n",
    "                  'Asymmetry (A)',\n",
    "                  'Clumpiness (S)',\n",
    "                  'Sersic N',\n",
    "                  'Shape Asymmetry (A_S)'))}\n",
    "\n",
    "#Counter\tImage\tMerger (0 = no, 1 = yes)\tMyr\tViewpoint\tGini\tM20\tC\tA\tS\tSersic n\n",
    "'''view=0\n",
    "df = pd.io.parsers.read_table(\n",
    "    filepath_or_buffer='PCA_img_0.txt',\n",
    "    header=[0],\n",
    "    sep='\\t', skiprows=14*view,nrows=14\n",
    "    )#,skiprows=10,nrows=10'''\n",
    "\n",
    "\n",
    "#list_runs=['fg3_m_12','fg1_m_13']\n",
    "list_runs=['fg3_m12']#,'fg1_m13']#,'fg1_m13']\n",
    "prior_list=[[0.94,0.06],[0.85,0.15],[0.06,0.94],[0.999,0.001],[0.001,0.999]]\n",
    "\n",
    "for k in range(len(prior_list)):\n",
    "    for i in range(len(list_runs)):\n",
    "   \n",
    "        add_on=list_runs[i]\n",
    "        print('run', add_on)\n",
    "\n",
    "\n",
    "        run=list_runs[i]\n",
    "        df = pd.io.parsers.read_table(\n",
    "            filepath_or_buffer='LDA_img_ratio_statmorph_'+str(run)+'.txt',#'_view_all.txt',#filepath_or_buffer='LDA_img_ratio_'+str(run)+'_early_late_all_things.txt',#'_view_all.txt',\n",
    "            header=[0],\n",
    "            sep='\\t'\n",
    "            )#,skiprows=10,nrows=10\n",
    "        ##filepath_or_buffer='LDA_img_ratio_statmorph_'+str(run)+'.txt',#'_view_all.txt',\n",
    "\n",
    "        df.columns = [l for i,l in sorted(feature_dict.items())] + ['Shape Asymmetry']\n",
    "        df.dropna(how=\"all\", inplace=True) # to drop the empty line at file-end\n",
    "\n",
    "        for j in range(len(df)):\n",
    "            if df[['Myr']].values[j][0]<40 and df[['Sep']].values[j][0]==0.0 and df[['# Bulges']].values[j][0]==1:#df[['Myr']].values[i][0]\n",
    "\n",
    "\n",
    "                #I use this part to check if there is any separation at these points in time\n",
    "                #Or if there are more than two bulges\n",
    "                #print(df[['class label','Myr','Viewpoint','# Bulges', 'Sep']].values[j])\n",
    "\n",
    "                #Then, you can optionally change the class values of all of these viewpoints\n",
    "\n",
    "                #.set_value(index, col, value, \n",
    "                df.set_value(j,'class label',0.0)\n",
    "\n",
    "\n",
    "\n",
    "        from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "        X = df[['Gini','M20','Concentration (C)', 'Asymmetry (A)', 'Clumpiness (S)', 'Sersic N', 'Shape Asymmetry']].values\n",
    "\n",
    "        from sklearn import preprocessing\n",
    "\n",
    "        #print('X before norm', X)\n",
    "\n",
    "        std_scale = preprocessing.StandardScaler().fit(X)\n",
    "        X = std_scale.transform(X)\n",
    "        #print('X after norm', X)\n",
    "\n",
    "        n_params=7\n",
    "\n",
    "\n",
    "        y = df['class label'].values\n",
    "\n",
    "\n",
    "        enc = LabelEncoder()\n",
    "        label_encoder = enc.fit(y)\n",
    "        y = label_encoder.transform(y) + 1\n",
    "\n",
    "\n",
    "        label_dict = {1: 'NonMerger', 2: 'Merger'}\n",
    "        from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "        # LDA\n",
    "        sklearn_lda = LDA(priors=prior_list[k])\n",
    "        X_lda_sklearn = sklearn_lda.fit_transform(X, y)\n",
    "        dec = sklearn_lda.score(X,y)\n",
    "        prob = sklearn_lda.predict_proba(X)\n",
    "\n",
    "        coef = sklearn_lda.coef_\n",
    "        inter = sklearn_lda.intercept_\n",
    "        class_label = sklearn_lda.classes_\n",
    "\n",
    "        print('~~~Coefficients and Intercepts~~~')\n",
    "        print(coef,inter)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        print('priors', prior_list[k])\n",
    "        print('mean accuracy',dec)#mean accuracy on the given test data and labels.\n",
    "\n",
    "        print(inter)\n",
    "        '''Make a histogram'''\n",
    "        from scipy import stats\n",
    "        import seaborn as sns\n",
    "        plt.clf()\n",
    "        fig = plt.figure(figsize=(18,10))\n",
    "        ax = fig.add_subplot(111)\n",
    "        X_lda_1=[]\n",
    "        X_lda_2=[]\n",
    "        for j in range(len(X_lda_sklearn)):\n",
    "            if y[j] ==1:\n",
    "                X_lda_1.append(X_lda_sklearn[j][0])\n",
    "            else:\n",
    "                X_lda_2.append(X_lda_sklearn[j][0])\n",
    "        input_hist=X_lda_sklearn\n",
    "\n",
    "        ax.hist(X_lda_1, label='NonMerger',  color=sns.xkcd_rgb[\"sky blue\"],alpha = 0.75)\n",
    "        ax.hist(X_lda_2, label='Merger',  color=sns.xkcd_rgb[\"salmon\"],alpha = 0.75)\n",
    "\n",
    "        '''for label,col in zip(range(1,4),  ('blue', 'red')):\n",
    "            input_hist=X_lda_sklearn\n",
    "            input_all=X_lda_sklearn\n",
    "            ax.hist(input_hist,\n",
    "                           color=col,\n",
    "                           label='class %s' %label_dict[label],\n",
    "                           alpha=0.5,)#bins=bins,\n",
    "            xt = plt.xticks()[0]  \n",
    "            xmin, xmax = -0.1,0.7#min(xt), max(xt)  \n",
    "            lnspc = np.linspace(xmin, xmax, len(input_hist))\n",
    "\n",
    "            # lets try the normal distribution first\n",
    "            m, s = stats.norm.fit(input_hist) # get mean and standard deviation  \n",
    "            pdf_g = stats.norm.pdf(lnspc, m, s) # now get theoretical values in our interval  \n",
    "            #ax.plot(lnspc, pdf_g,  color=col) # plot it\n",
    "\n",
    "\n",
    "\n",
    "        ylims = ax.get_ylim()\n",
    "\n",
    "        # plot annotation\n",
    "        leg = ax.legend(loc='upper right', fancybox=True, fontsize=8)\n",
    "        leg.get_frame().set_alpha(0.5)\n",
    "        ax.set_ylim([0, max(ylims)+2])'''\n",
    "\n",
    "        ax.set_xlabel('LD1', size=20)\n",
    "        #ax.set_title('Histogram #%s' %str(cnt+1), size=20)\n",
    "\n",
    "        # hide axis ticks\n",
    "        ax.tick_params(axis=\"both\", which=\"both\", bottom=\"off\", top=\"off\",  \n",
    "                labelbottom=\"on\", left=\"off\", right=\"off\", labelleft=\"on\", labelsize=20)\n",
    "\n",
    "        # remove axis spines\n",
    "        ax.spines[\"top\"].set_visible(False)  \n",
    "        ax.spines[\"right\"].set_visible(False)\n",
    "        ax.spines[\"bottom\"].set_visible(False)\n",
    "        ax.spines[\"left\"].set_visible(False)    \n",
    "\n",
    "        ax.set_ylabel('Count', size=20)\n",
    "\n",
    "\n",
    "        plt.legend(loc=\"upper right\", fontsize=20)\n",
    "        #fig.tight_layout() \n",
    "        #plt.annotate(str(add_on), xy=(0.02,0.95),xycoords='axes fraction', size=20)\n",
    "        #plt.annotate('Mean Accuracy = '+str(dec), xy=(0.02,0.9),xycoords='axes fraction', size=20)\n",
    "        #frame1 = plt.gca()\n",
    "        if run=='fg1_m_13':\n",
    "            plt.title('FG1M13')\n",
    "        if run=='fg3_m12':\n",
    "            plt.title('FG3M12')\n",
    "        plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
